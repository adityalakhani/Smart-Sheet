================================================================
RepopackPy Output File
================================================================

This file was generated by RepopackPy on: 2025-09-22T01:03:30.632522

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and RepopackPy's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

For more information about RepopackPy, visit: https://github.com/abinthomasonline/repopack-py

================================================================
Repository Structure
================================================================
LICENSE
README.md
agents\__init__.py
agents\base_agent.py
agents\interviewer_agent.py
agents\quecraft_agent.py
agents\recruiter_agent.py
agents\reviewer_agent.py
app.py
config.py
llm_clients.py
orchestrator.py
requirements.txt
utils\__init__.py
utils\json_parser.py
utils\mock_data_generator.py
utils\prompts.py
utils\report_generator.py
utils\tool_logger.py
utils\web_search.py

================================================================
Repository Files
================================================================

================
File: app.py
================
import streamlit as st
import logging
from datetime import datetime
import json
from typing import Dict, Any

from orchestrator import InterviewOrchestrator
from config import Config
from utils.tool_logger import tool_logger_instance

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Page configuration
st.set_page_config(
    page_title="Adaptive Excel Skills Assessment",
    page_icon=":bar_chart:",
    layout="wide",
    initial_sidebar_state="expanded"
)

def initialize_session_state():
    """Initialize Streamlit session state variables"""
    if "orchestrator" not in st.session_state:
        st.session_state.orchestrator = None
    if "interview_active" not in st.session_state:
        st.session_state.interview_active = False
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []
    if "current_question" not in st.session_state:
        st.session_state.current_question = None
    if "interview_started" not in st.session_state:
        st.session_state.interview_started = False
    if "candidate_name" not in st.session_state:
        st.session_state.candidate_name = ""
    if "interview_completed" not in st.session_state:
        st.session_state.interview_completed = False
    if "excel_skill_focus" not in st.session_state:
        st.session_state.excel_skill_focus = []
    if "current_dataset" not in st.session_state:
        st.session_state.current_dataset = None

def display_header():
    """Display adaptive header"""
    st.title("Adaptive Microsoft Excel Skills Assessment")
    st.markdown("### AI-Powered Response-Driven Interview System")
    
    st.markdown("""
        ### Adaptive Features:

        - **Response-Driven Questions** - Questions adapt based on your performance  
        - **Dynamic Difficulty Adjustment** - Assessment level changes based on your demonstrated skills  
        - **Personalized Learning Path** - Each candidate gets a unique question sequence  
        - **Real-Time Skill Discovery** - System identifies strengths and areas for improvement  
        - **Contextual Dataset Generation** - Realistic business data matched to each scenario  

        ---

        ### Assessment Format:

        - **Adaptive Questioning** - No fixed sequence; questions generated based on your responses  
        - **Performance-Based Trajectory** - Assessment path changes based on demonstrated capabilities  
        - **Case-Based Scenarios** - Realistic business situations requiring Excel solutions  
        - **Methodology Focus** - Explain your approach; no software execution required  
        - **Intelligent Question Generation** - Maximum of 2 questions prepared at a time  
        - **Comprehensive Coverage** - Tests formulas, pivot tables, data analysis, and more  
    """)

def display_sidebar():
    """Display adaptive sidebar"""
    with st.sidebar:
        st.header("Adaptive Assessment Setup")
        
        # Excel skill focus areas
        st.subheader("Excel Skill Preferences")
        skill_options = [
            "Basic Formulas & Functions",
            "Data Manipulation & Cleaning", 
            "VLOOKUP & Lookup Functions",
            "Pivot Tables & Analysis",
            "Data Visualization", 
            "Advanced Functions",
            "Error Handling & Validation",
            "Conditional Logic"
        ]
        
        if not st.session_state.interview_started:
            st.session_state.candidate_name = st.text_input(
                "Your Name", 
                value=st.session_state.candidate_name,
                placeholder="Enter your full name"
            )
            
            role_context = st.selectbox(
                "Role Context",
                ["Business Analyst", "Data Analyst", "Financial Analyst", 
                 "Project Manager", "Operations Manager", "General Business User"],
                index=0
            )
            
            proficiency_level = st.selectbox(
                "Initial Proficiency Level",
                ["Beginner", "Intermediate", "Advanced", "Mixed Assessment"],
                index=3,
                help="Starting point for adaptive assessment - system will adjust based on your responses"
            )
            
            st.session_state.excel_skill_focus = st.multiselect(
                "Areas of Interest (optional)",
                skill_options,
                default=[],
                help="Optional focus areas - system will adapt based on performance"
            )
            
            st.subheader("Adaptive Configuration")
            max_questions = st.slider(
                "Maximum Questions", 
                6, 15, 10,
                help="Upper limit - actual number depends on your responses"
            )
            include_datasets = st.checkbox(
                "Enable Contextual Datasets", 
                True, 
                help="Generate realistic datasets for each question scenario"
            )
            
            if st.button("Start Adaptive Assessment", 
                        type="primary", 
                        disabled=not st.session_state.candidate_name.strip()):
                start_adaptive_assessment(role_context, proficiency_level, max_questions, include_datasets)
        
        else:
            # Assessment in progress
            st.write(f"**Candidate:** {st.session_state.candidate_name}")
            
            # Adaptive progress tracking
            if st.session_state.orchestrator:
                progress = st.session_state.orchestrator.get_progress()
                
                st.write(f"**Questions Completed:** {progress['questions_completed']}")
                st.write(f"**Current Question:** {progress['current_question_number']}")
                st.write(f"**Assessment Mode:** Adaptive")
                
                # Current skill and adaptive info
                if st.session_state.current_question:
                    skill = st.session_state.current_question.get('skill_target', 'N/A')
                    difficulty = st.session_state.current_question.get('difficulty', 'N/A')
                    has_dataset = st.session_state.current_question.get('requires_dataset', False)
                    
                    st.write(f"**Current Focus:** {skill}")
                    st.write(f"**Difficulty:** {difficulty}")
                    if has_dataset:
                        st.write("**Dataset:** Available")
                    
                    # Display dataset toggle
                    if has_dataset and st.button("View Current Dataset"):
                        load_current_dataset()
                
                # Adaptive insights
                if 'candidate_profile' in progress:
                    profile = progress['candidate_profile']
                    st.markdown("**Adaptive Profile:**")
                    st.write(f"- Preferred Level: {profile.get('preferred_difficulty', 'Determining')}")
                    st.write(f"- Strengths: {len(profile.get('strengths', []))}")
                    st.write(f"- Focus Areas: {len(profile.get('areas_needing_focus', []))}")
                    st.write(f"- Trend: {progress.get('performance_trend', 'Analyzing').title()}")
                    st.write(f"- Adaptations: {progress.get('trajectory_decisions', 0)}")
            
            # Assessment controls
            st.markdown("---")
            col1, col2 = st.columns(2)
            
            with col1:
                if st.button("Hint", disabled=not st.session_state.current_question):
                    provide_methodology_hint()
            
            with col2:
                if st.button("Reset", type="secondary"):
                    if st.button("Confirm Reset", type="secondary"):
                        reset_assessment()
        
        # System information
        st.markdown("---")
        st.subheader("System Status")
        
        if st.button("Tool Performance"):
            show_tool_performance()
        
        st.write("**AI Model:** Gemini 2.0 Flash")
        st.write("**Mode:** Adaptive Assessment")
        st.write("**Question Generation:** Dynamic")
        st.write("**Status:** Ready")

def start_adaptive_assessment(role_context: str, proficiency_level: str, 
                             max_questions: int, include_datasets: bool):
    """Initialize adaptive assessment"""
    try:
        with st.spinner("Initializing Adaptive Excel Assessment System..."):
            # Initialize adaptive orchestrator
            st.session_state.orchestrator = InterviewOrchestrator()
            
            # Start adaptive interview
            result = st.session_state.orchestrator.start_interview(
                candidate_name=st.session_state.candidate_name,
                role_type=role_context,
                difficulty_level=proficiency_level,
                excel_focus_areas=st.session_state.excel_skill_focus,
                max_questions=max_questions,
                include_datasets=include_datasets
            )
            
            if result["success"]:
                st.session_state.interview_active = True
                st.session_state.interview_started = True
                st.session_state.current_question = result.get("first_question")
                
                # Adaptive welcome message
                welcome_msg = result.get("welcome_message", 
                    f"Welcome {st.session_state.candidate_name} to your adaptive Excel skills assessment!")
                st.session_state.chat_history.append({
                    "role": "assistant", 
                    "content": welcome_msg,
                    "type": "adaptive_welcome"
                })
                
                # Add first question with adaptive info
                if st.session_state.current_question:
                    question_text = st.session_state.current_question.get("question", "")
                    
                    question_entry = {
                        "role": "assistant", 
                        "content": question_text,
                        "type": "adaptive_question",
                        "question_data": st.session_state.current_question
                    }
                    
                    # Include dataset information
                    if st.session_state.current_question.get("dataset_info", {}).get("generated_successfully"):
                        question_entry["has_dataset"] = True
                        question_entry["dataset_info"] = st.session_state.current_question["dataset_info"]
                    
                    st.session_state.chat_history.append(question_entry)
                
                st.success(f"Adaptive Excel assessment started! Questions will be generated based on your responses. Initial questions prepared: {result.get('questions_prepared', 2)}")
                st.rerun()
                
            else:
                st.error(f"Failed to start adaptive assessment: {result.get('error', 'Unknown error')}")
                
    except Exception as e:
        logger.error(f"Error starting adaptive Excel assessment: {str(e)}")
        st.error(f"Error starting assessment: {str(e)}")

def load_current_dataset():
    """Load and display the current question's dataset"""
    try:
        if st.session_state.orchestrator:
            dataset_result = st.session_state.orchestrator.get_current_question_dataset()
            if dataset_result["success"]:
                st.session_state.current_dataset = dataset_result
            else:
                st.error(f"Failed to load dataset: {dataset_result.get('error')}")
    except Exception as e:
        st.error(f"Error loading dataset: {str(e)}")

def display_chat_interface():
    """Display adaptive chat interface with dataset support"""
    st.subheader("Adaptive Excel Assessment Conversation")
    
    # Chat container with adaptive styling
    chat_container = st.container()
    with chat_container:
        for i, message in enumerate(st.session_state.chat_history):
            message_type = message.get("type", "general")
            
            with st.chat_message(message["role"]):
                if message_type == "adaptive_question":
                    # Adaptive question display
                    question_data = message.get("question_data", {})
                    
                    st.markdown(f"### Question {i//2 + 1}: {question_data.get('skill_target', 'Excel Challenge')}")
                    
                    if question_data.get("difficulty"):
                        difficulty_colors = {"Easy": "Green", "Medium": "Orange", "Hard": "Red"}
                        difficulty_color = difficulty_colors.get(question_data["difficulty"], "Gray")
                        st.markdown(f"**Difficulty:** {difficulty_color} {question_data['difficulty']}")
                    
                    # Show adaptive reasoning if available
                    if question_data.get("adaptive_reasoning"):
                        st.info(f"**Why this question:** {question_data['adaptive_reasoning']}")
                    
                    st.markdown(message["content"])
                    
                    # Dataset display
                    if message.get("has_dataset") and message.get("dataset_info", {}).get("generated_successfully"):
                        with st.expander("View Contextual Dataset", expanded=False):
                            dataset_info = message["dataset_info"]
                            
                            # Display context analysis
                            if dataset_info.get("context_analysis"):
                                context = dataset_info["context_analysis"]
                                st.info(f"**Dataset Context:** {context.get('question_type', 'Business Scenario')}")
                            
                            # Display HTML table
                            if dataset_info.get("html_table"):
                                st.markdown("**Dataset:**")
                                st.markdown(dataset_info["html_table"], unsafe_allow_html=True)
                            
                            # Dataset metadata
                            if dataset_info.get("metadata"):
                                metadata = dataset_info["metadata"]
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Rows", metadata.get("rows", 0))
                                with col2:
                                    st.metric("Columns", metadata.get("columns", 0))
                                with col3:
                                    if metadata.get("has_missing_values"):
                                        st.metric("Data Issues", "Yes", help="Contains missing/inconsistent data for practice")
                
                elif message_type == "hint":
                    st.info(f"Hint: {message['content']}")
                
                elif message_type == "adaptive_welcome":
                    st.success(message["content"])
                
                else:
                    st.markdown(message["content"])
    
    # Adaptive response input
    if st.session_state.interview_active and not st.session_state.interview_completed:
        st.markdown("### Your Excel Methodology Response")
        st.markdown("*Describe your step-by-step approach, Excel functions you'd use, and your reasoning. Reference the dataset if provided.*")
        
        # Show dataset reminder if current question has data
        if (st.session_state.current_question and 
            st.session_state.current_question.get("requires_dataset") and
            st.session_state.current_question.get("dataset_info", {}).get("generated_successfully")):
            st.info("Dataset Available: Use the 'View Current Dataset' button in the sidebar to reference the data while formulating your response.")
        
        user_input = st.chat_input("Explain your Excel approach here...")
        
        if user_input:
            # Add user message
            st.session_state.chat_history.append({
                "role": "user", 
                "content": user_input,
                "type": "adaptive_methodology_response"
            })
            
            # Process adaptive response
            with st.spinner("Processing your response and generating adaptive follow-up..."):
                process_adaptive_response(user_input)
            
            st.rerun()
    
    elif st.session_state.interview_completed:
        st.success("Adaptive Excel assessment completed! Check your personalized results below.")
    
    elif not st.session_state.interview_started:
        st.info("Please configure your adaptive assessment settings and start the evaluation.")

def process_adaptive_response(user_input: str):
    """Process candidate's response with adaptive features"""
    try:
        if st.session_state.orchestrator:
            result = st.session_state.orchestrator.process_response(user_input)
            
            if result["success"]:
                # Add adaptive acknowledgment
                if result.get("acknowledgment"):
                    st.session_state.chat_history.append({
                        "role": "assistant", 
                        "content": result["acknowledgment"],
                        "type": "adaptive_acknowledgment"
                    })
                
                # Check assessment completion
                if result.get("interview_complete"):
                    st.session_state.interview_completed = True
                    st.session_state.interview_active = False
                    
                    # Add adaptive conclusion
                    if result.get("conclusion_message"):
                        st.session_state.chat_history.append({
                            "role": "assistant", 
                            "content": result["conclusion_message"],
                            "type": "adaptive_conclusion"
                        })
                
                else:
                    # Add next adaptive question
                    if result.get("next_question"):
                        st.session_state.current_question = result["next_question"]
                        question_text = result["next_question"].get("question", "")
                        
                        question_entry = {
                            "role": "assistant", 
                            "content": question_text,
                            "type": "adaptive_question",
                            "question_data": result["next_question"]
                        }
                        
                        # Include adaptive dataset info
                        if result.get("next_question_has_dataset") and result.get("dataset_info"):
                            question_entry["has_dataset"] = True
                            question_entry["dataset_info"] = result["dataset_info"]
                        
                        st.session_state.chat_history.append(question_entry)
            
            else:
                st.error(f"Error processing response: {result.get('error', 'Unknown error')}")
                
    except Exception as e:
        logger.error(f"Error processing adaptive Excel response: {str(e)}")
        st.error(f"Error processing response: {str(e)}")

def display_adaptive_results():
    """Display comprehensive adaptive Excel assessment results"""
    if st.session_state.interview_completed and st.session_state.orchestrator:
        st.markdown("---")
        st.header("Adaptive Excel Skills Assessment Results")
        
        try:
            results = st.session_state.orchestrator.get_final_results()
            
            if results["success"]:
                final_decision = results["final_decision"]
                performance_metrics = results["performance_metrics"]
                adaptive_features = results.get("adaptive_features", {})
                
                # Adaptive results display
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    score = final_decision.get('overall_score', 0)
                    st.metric("Excel Proficiency Score", f"{score}/100", 
                             help="Adaptive assessment score based on personalized trajectory")
                
                with col2:
                    decision = final_decision.get('decision', 'N/A')
                    st.metric("Assessment Result", decision)
                
                with col3:
                    st.metric("Questions Completed", performance_metrics.get('total_questions', 0))
                
                with col4:
                    adaptations = adaptive_features.get('questions_adapted', 0)
                    st.metric("Adaptive Adjustments", adaptations, 
                             help="Number of times questions were adapted based on your responses")
                
                # Adaptive analytics tabs
                tab1, tab2, tab3, tab4, tab5 = st.tabs([
                    "Adaptive Summary", 
                    "Skill Assessment", 
                    "Learning Trajectory",
                    "Detailed Feedback",
                    "System Performance"
                ])
                
                with tab1:
                    st.markdown("### Adaptive Excel Assessment Summary")
                    st.write(final_decision.get('recommendation_summary', 'No summary available'))
                    
                    # Adaptive insights
                    candidate_profile = results.get("candidate_profile", {})
                    
                    st.markdown("#### Adaptive Learning Insights")
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.info(f"**Learning Trajectory:** {adaptive_features.get('questions_adapted', 0)} adaptations made")
                        st.info(f"**Final Difficulty Level:** {adaptive_features.get('final_difficulty_level', 'Medium')}")
                        
                    with col2:
                        st.info(f"**Skills Discovered:** {adaptive_features.get('skills_discovered', 0)}")
                        trend = adaptive_features.get('performance_trend', 'stable')
                        trend_indicators = {"improving": "Upward", "declining": "Downward", "stable": "Consistent"}
                        st.info(f"**Performance Trend:** {trend_indicators.get(trend, 'Stable')} {trend.title()}")
                    
                    # Strengths and areas for improvement
                    col1, col2 = st.columns(2)
                    with col1:
                        st.markdown("**Excel Strengths:**")
                        strengths = candidate_profile.get('strengths', [])
                        if strengths:
                            for strength in strengths:
                                st.write(f"• {strength}")
                        else:
                            st.write("• Strengths still being assessed")
                    
                    with col2:
                        st.markdown("**Areas for Development:**")
                        focus_areas = candidate_profile.get('areas_needing_focus', [])
                        if focus_areas:
                            for area in focus_areas:
                                st.write(f"• {area}")
                        else:
                            st.write("• Well-rounded performance across all areas")
                
                with tab2:
                    st.markdown("### Excel Skill Area Assessment")
                    
                    skill_assessment = final_decision.get('skill_assessment', {})
                    for skill, rating in skill_assessment.items():
                        skill_name = skill.replace('_', ' ').title()
                        
                        rating_values = {
                            "Excellent": 95, "Good": 80, "Fair": 65, "Poor": 40
                        }
                        
                        value = rating_values.get(rating, 50)
                        st.progress(value / 100, text=f"{skill_name}: {rating}")
                
                with tab3:
                    st.markdown("### Adaptive Learning Trajectory")
                    
                    trajectory_decisions = adaptive_features.get('trajectory_decisions', [])
                    if trajectory_decisions:
                        st.markdown("#### How the Assessment Adapted to You:")
                        for i, decision in enumerate(trajectory_decisions, 1):
                            with st.expander(f"Adaptation {i}: {decision.get('timestamp', 'Unknown time')}"):
                                st.write(f"**Reasoning:** {decision.get('reasoning', 'Adaptive adjustment made')}")
                                st.write(f"**Trajectory:** {decision.get('trajectory', 'Continued assessment')}")
                                st.write(f"**Questions Generated:** {decision.get('questions_generated', 'N/A')}")
                    else:
                        st.info("Assessment completed without major trajectory changes.")
                    
                    # Profile evolution metrics
                    st.markdown("#### Your Excel Profile Evolution")
                    profile_metrics = performance_metrics.get("candidate_profile_evolution", {})
                    
                    if profile_metrics:
                        col1, col2, col3 = st.columns(3)
                        with col1:
                            st.metric("Strengths Identified", profile_metrics.get("strengths_identified", 0))
                        with col2:
                            st.metric("Areas Improved", profile_metrics.get("areas_improved", 0))
                        with col3:
                            st.metric("Final Level", profile_metrics.get("final_difficulty_level", "Medium"))
                
                with tab4:
                    if st.button("Generate Adaptive Feedback Report"):
                        with st.spinner("Creating personalized adaptive assessment report..."):
                            feedback_report = st.session_state.orchestrator.generate_feedback_report()
                            if feedback_report["success"]:
                                st.text_area(
                                    "Adaptive Excel Skills Assessment Report",
                                    feedback_report["report"],
                                    height=600,
                                    help="Comprehensive feedback including adaptive learning analysis"
                                )
                                
                                # Download report
                                st.download_button(
                                    label="Download Adaptive Assessment Report",
                                    data=feedback_report["report"],
                                    file_name=f"adaptive_excel_assessment_{st.session_state.candidate_name.replace(' ', '_')}.txt",
                                    mime="text/plain"
                                )
                
                with tab5:
                    show_adaptive_system_performance()
            
            else:
                st.error("Unable to load adaptive Excel assessment results")
                
        except Exception as e:
            logger.error(f"Error displaying adaptive Excel results: {str(e)}")
            st.error(f"Error loading adaptive results: {str(e)}")

def show_adaptive_system_performance():
    """Show adaptive system performance metrics"""
    st.markdown("### Adaptive System Performance During Assessment")
    
    performance_report = tool_logger_instance.get_performance_report()
    
    # Adaptive summary metrics
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Tool Calls", performance_report["summary"]["total_calls"])
    with col2:
        st.metric("Success Rate", f"{performance_report['summary']['success_rate']:.1f}%")
    with col3:
        st.metric("Tools Used", performance_report["summary"]["tools_used"])
    with col4:
        # Count adaptive question generation calls
        adaptive_calls = len([call for call in tool_logger_instance.call_history 
                             if 'adaptive' in call.get('tool_name', '').lower() or 
                                'generate' in call.get('tool_name', '').lower()])
        st.metric("Adaptive Generations", adaptive_calls, help="Dynamic questions and data generated")
    
    # Tool breakdown with adaptive context
    if performance_report["per_tool_metrics"]:
        st.markdown("#### Tool Usage Breakdown")
        
        for tool_name, metrics in performance_report["per_tool_metrics"].items():
            with st.expander(f"{tool_name} - {metrics['successful_calls']}/{metrics['total_calls']} calls"):
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write(f"**Total Calls:** {metrics['total_calls']}")
                    st.write(f"**Successful:** {metrics['successful_calls']}")
                    st.write(f"**Failed:** {metrics['failed_calls']}")
                
                with col2:
                    st.write(f"**Avg Execution Time:** {metrics['average_execution_time']:.2f}s")
                    success_rate = (metrics['successful_calls'] / metrics['total_calls'] * 100) if metrics['total_calls'] > 0 else 0
                    st.write(f"**Success Rate:** {success_rate:.1f}%")
                
                # Special handling for adaptive tools
                if any(keyword in tool_name.lower() for keyword in ['generate', 'adaptive', 'question']):
                    st.info("This tool supports adaptive question generation and personalization")

def provide_methodology_hint():
    """Provide Excel methodology hint with adaptive context"""
    if st.session_state.orchestrator and st.session_state.current_question:
        try:
            hint_result = st.session_state.orchestrator.provide_hint("methodology_focused")
            if hint_result["success"]:
                hint_msg = f"Excel Methodology Hint: {hint_result['hint_message']}"
                st.session_state.chat_history.append({
                    "role": "assistant", 
                    "content": hint_msg,
                    "type": "adaptive_hint"
                })
                st.rerun()
        except Exception as e:
            st.error(f"Error providing hint: {str(e)}")

def reset_assessment():
    """Reset the adaptive assessment session"""
    # Clear session state
    for key in ["orchestrator", "interview_active", "chat_history", "current_question", 
                "interview_started", "interview_completed", "excel_skill_focus", "current_dataset"]:
        if key in st.session_state:
            del st.session_state[key]
    
    initialize_session_state()
    st.rerun()

def show_tool_performance():
    """Display tool performance metrics in sidebar"""
    performance_report = tool_logger_instance.get_performance_report()
    
    with st.expander("Adaptive Tool Performance Metrics"):
        st.json(performance_report["summary"])
        
        if performance_report["per_tool_metrics"]:
            st.write("**Tools Used in Adaptive Mode:**")
            for tool_name, metrics in performance_report["per_tool_metrics"].items():
                st.write(f"- {tool_name}: {metrics['successful_calls']}/{metrics['total_calls']} successful")

def display_adaptive_tips():
    """Display adaptive Excel methodology tips"""
    if st.session_state.interview_active:
        st.markdown("### Adaptive Excel Assessment Tips")
        tips = [
            "Questions adapt based on your performance - there's no fixed sequence",
            "Reference provided datasets when explaining your approach",
            "Think step-by-step and explain your Excel methodology clearly",
            "The system learns from your responses to generate better questions",
            "Don't worry about perfect syntax - focus on logical problem-solving",
            "Each question builds on what the system learned about your skills",
            "Your performance determines the difficulty of subsequent questions"
        ]
        for tip in tips:
            st.write(f"• {tip}")
    
    elif st.session_state.interview_completed:
        st.markdown("### Your Adaptive Assessment Journey")
        if st.session_state.orchestrator:
            progress = st.session_state.orchestrator.get_progress()
            candidate_profile = progress.get('candidate_profile', {})
            
            next_steps = [
                f"Focus on {candidate_profile.get('preferred_difficulty', 'appropriate')} level Excel challenges",
                "Practice areas identified as needing development",
                "Build on strengths discovered during the assessment",
                "Work with realistic business data scenarios",
                "Consider Excel certification at your demonstrated level"
            ]
            for step in next_steps:
                st.write(f"• {step}")

def display_footer():
    """Display adaptive application footer"""
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: #666; padding: 20px;'>
            <p><strong>Adaptive Microsoft Excel Skills Assessment System</strong></p>
            <p>Powered by AI • Dynamic Question Generation • Personalized Learning Paths</p>
            <p>Your responses are processed securely and used only for assessment purposes</p>
            <p><em>Assessment adapts to your performance in real-time</em></p>
        </div>
        """,
        unsafe_allow_html=True
    )

def main():
    """Main adaptive application function"""
    # Initialize session state
    initialize_session_state()
    
    # Display adaptive header
    display_header()
    
    # Main layout
    col1, col2 = st.columns([3, 1])
    
    with col1:
        # Main adaptive assessment interface
        display_chat_interface()
        display_adaptive_results()
    
    with col2:
        # Adaptive tips in main area
        display_adaptive_tips()
    
    # Adaptive sidebar
    display_sidebar()
    
    # Footer
    display_footer()

if __name__ == "__main__":
    main()

================
File: config.py
================
"""
Configuration for AI-Powered Excel Mock Interviewer
Focused specifically on Microsoft Excel skills assessment
"""

import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class Config:
    """Configuration settings for Excel Mock Interviewer"""
    
    # API Keys
    GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
    
    # Model Configuration - Focused on Excel assessment
    GEMINI_PRO_MODEL = "gemini-2.0-flash-exp"  # For complex reasoning (QueCraft, Reviewer, Recruiter)
    GEMINI_LITE_MODEL = "gemini-2.0-flash-exp"  # For conversational tasks (Interviewer)
    
    # Rate Limiting
    PRO_MODEL_RATE_LIMIT = 15  # Requests per minute
    LITE_MODEL_RATE_LIMIT = 60  # Requests per minute
    
    # Interview Configuration - Excel Focused
    MIN_QUESTIONS_THRESHOLD = 5
    MAX_QUESTIONS_PER_INTERVIEW = 10
    
    # Excel Skill Categories - Core Focus Areas
    EXCEL_SKILL_CATEGORIES = [
        "Basic Formulas and Functions",
        "Data Manipulation and Cleaning", 
        "Lookup Functions (VLOOKUP, INDEX/MATCH)",
        "Pivot Tables and Data Analysis",
        "Data Visualization and Charts",
        "Conditional Logic and IF Statements",
        "Text Functions and String Manipulation",
        "Advanced Functions and Array Formulas",
        "Data Validation and Error Handling",
        "Macro Basics and Automation"
    ]
    
    # Difficulty Levels
    DIFFICULTY_LEVELS = ["Easy", "Medium", "Hard", "Mixed"]
    
    # Assessment Focus - Excel Only
    ASSESSMENT_FOCUS = "microsoft_excel_proficiency"
    
    # Data Generation Configuration
    DATA_GENERATION_METHOD = "faker_synthetic"  # Use Faker library
    DEFAULT_DATASET_SIZE = 50  # Default number of records
    MAX_DATASET_SIZE = 200     # Maximum records for performance
    
    # Supported Data Types for Excel Questions
    EXCEL_DATA_TYPES = [
        "sales_transactions",
        "employee_records", 
        "financial_expenses",
        "inventory_stock",
        "customer_database",
        "survey_responses",
        "project_timeline",
        "budget_allocation",
        "performance_metrics",
        "contact_information"
    ]
    
    # Question Types - Case-Based Approach
    QUESTION_FORMATS = [
        "case_study",      # Scenario-based questions
        "problem_solving", # How would you approach...
        "best_practice",   # What's the best way to...
        "troubleshooting", # How would you fix...
        "optimization"     # How would you improve...
    ]
    
    # Logging Configuration
    ENABLE_TOOL_LOGGING = True
    LOG_LEVEL = "INFO"
    LOG_FORMAT = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    
    # Report Configuration
    GENERATE_DETAILED_REPORTS = True
    INCLUDE_SKILL_BREAKDOWN = True
    
    @classmethod
    def validate_config(cls):
        """Validate configuration settings"""
        if not cls.GOOGLE_API_KEY:
            raise ValueError("GOOGLE_API_KEY environment variable is required")
        
        return True

================
File: LICENSE
================
MIT License

Copyright (c) 2025 Aditya Lakhani

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: llm_clients.py
================
"""
LLM Client management for Google Gemini models
Handles both Pro and Flash Lite clients with rate limiting and error handling
"""

import time
import logging
from typing import Dict, Any, Optional
from google import genai
from config import Config
import threading
from collections import defaultdict
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)

class RateLimiter:
    """
    Simple rate limiter for API calls
    """
    
    def __init__(self, max_requests: int, time_window: int = 60):
        """
        Initialize rate limiter
        
        Args:
            max_requests (int): Maximum requests allowed in time window
            time_window (int): Time window in seconds (default: 60 seconds)
        """
        self.max_requests = max_requests
        self.time_window = time_window
        self.requests = []
        self.lock = threading.Lock()
    
    def can_make_request(self) -> bool:
        """
        Check if a request can be made without exceeding rate limit
        
        Returns:
            bool: True if request can be made
        """
        with self.lock:
            now = datetime.now()
            # Remove old requests outside time window
            self.requests = [req_time for req_time in self.requests 
                           if now - req_time < timedelta(seconds=self.time_window)]
            
            return len(self.requests) < self.max_requests
    
    def record_request(self):
        """Record a new request"""
        with self.lock:
            self.requests.append(datetime.now())
    
    def wait_if_needed(self):
        """Wait if rate limit would be exceeded"""
        while not self.can_make_request():
            time.sleep(1)
        self.record_request()

class GeminiClient:
    """
    Wrapper for Google Gemini client with enhanced error handling and logging
    """
    
    def __init__(self, model_name: str, rate_limiter: RateLimiter):
        """
        Initialize Gemini client
        
        Args:
            model_name (str): Name of the Gemini model
            rate_limiter (RateLimiter): Rate limiter instance
        """
        self.model_name = model_name
        self.rate_limiter = rate_limiter
        self.client = genai.Client(api_key=Config.GOOGLE_API_KEY)
        self.request_count = 0
        self.error_count = 0
        
        logger.info(f"Initialized Gemini client for model: {model_name}")
    
    def generate_content(self, contents: str, **kwargs) -> Any:
        """
        Generate content using Gemini model with rate limiting and error handling
        
        Args:
            contents (str): Input content/prompt
            **kwargs: Additional parameters for the API call
            
        Returns:
            Response object from Gemini API
        """
        try:
            # Apply rate limiting
            self.rate_limiter.wait_if_needed()
            
            # Make API call
            start_time = time.time()
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=contents,
                **kwargs
            )
            
            # Log successful request
            duration = time.time() - start_time
            self.request_count += 1
            
            logger.debug(f"Successful API call to {self.model_name} "
                        f"(duration: {duration:.2f}s, count: {self.request_count})")
            
            return response
            
        except Exception as e:
            self.error_count += 1
            logger.error(f"Error calling {self.model_name}: {str(e)} "
                        f"(error count: {self.error_count})")
            raise
    
    def get_stats(self) -> Dict[str, Any]:
        """
        Get client statistics
        
        Returns:
            dict: Client usage statistics
        """
        return {
            "model_name": self.model_name,
            "request_count": self.request_count,
            "error_count": self.error_count,
            "success_rate": (self.request_count - self.error_count) / max(self.request_count, 1) * 100
        }

class LLMClients:
    """
    Manager for both Pro and Lite Gemini clients
    """
    
    def __init__(self):
        """Initialize both Pro and Lite clients with separate rate limiters"""
        try:
            # Configure genai with API key
            # genai.configure(api_key=Config.GOOGLE_API_KEY)
            
            # Create rate limiters for each model
            self.pro_rate_limiter = RateLimiter(
                max_requests=Config.PRO_MODEL_RATE_LIMIT,
                time_window=60
            )
            
            self.lite_rate_limiter = RateLimiter(
                max_requests=Config.LITE_MODEL_RATE_LIMIT,
                time_window=60
            )
            
            # Initialize clients
            self.pro_client = GeminiClient(
                Config.GEMINI_PRO_MODEL,
                self.pro_rate_limiter
            )
            
            self.lite_client = GeminiClient(
                Config.GEMINI_LITE_MODEL,
                self.lite_rate_limiter
            )
            
            # Test connectivity
            self._test_connectivity()
            
            logger.info("LLM clients initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize LLM clients: {str(e)}")
            raise
    
    def _test_connectivity(self):
        """Test connectivity to both models"""
        try:
            # Test Pro model
            test_response = self.pro_client.generate_content("Hello, respond with 'OK'")
            logger.info(f"Pro model connectivity test: {test_response.text[:20] if test_response.text else 'No response'}")
            
            # Small delay between tests
            time.sleep(1)
            
            # Test Lite model
            test_response = self.lite_client.generate_content("Hello, respond with 'OK'")
            logger.info(f"Lite model connectivity test: {test_response.text[:20] if test_response.text else 'No response'}")
            
        except Exception as e:
            logger.warning(f"Connectivity test failed: {str(e)}")
            # Don't raise here as the models might still work for actual requests
    
    def get_pro_client(self) -> GeminiClient:
        """
        Get the Pro model client (for complex reasoning tasks)
        
        Returns:
            GeminiClient: Pro model client
        """
        return self.pro_client
    
    def get_lite_client(self) -> GeminiClient:
        """
        Get the Lite model client (for conversational tasks)
        
        Returns:
            GeminiClient: Lite model client
        """
        return self.lite_client
    
    def get_client_stats(self) -> Dict[str, Any]:
        """
        Get statistics for both clients
        
        Returns:
            dict: Combined statistics
        """
        return {
            "pro_client": self.pro_client.get_stats(),
            "lite_client": self.lite_client.get_stats(),
            "total_requests": self.pro_client.request_count + self.lite_client.request_count,
            "total_errors": self.pro_client.error_count + self.lite_client.error_count
        }
    
    def health_check(self) -> Dict[str, Any]:
        """
        Perform health check on both clients
        
        Returns:
            dict: Health status
        """
        health_status = {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "healthy",
            "clients": {}
        }
        
        # Check Pro client
        try:
            pro_stats = self.pro_client.get_stats()
            pro_healthy = pro_stats["success_rate"] > 80  # 80% success rate threshold
            
            health_status["clients"]["pro"] = {
                "status": "healthy" if pro_healthy else "degraded",
                "stats": pro_stats,
                "rate_limit_status": "ok" if self.pro_rate_limiter.can_make_request() else "limited"
            }
            
        except Exception as e:
            health_status["clients"]["pro"] = {
                "status": "unhealthy",
                "error": str(e)
            }
            health_status["overall_status"] = "degraded"
        
        # Check Lite client
        try:
            lite_stats = self.lite_client.get_stats()
            lite_healthy = lite_stats["success_rate"] > 80
            
            health_status["clients"]["lite"] = {
                "status": "healthy" if lite_healthy else "degraded", 
                "stats": lite_stats,
                "rate_limit_status": "ok" if self.lite_rate_limiter.can_make_request() else "limited"
            }
            
        except Exception as e:
            health_status["clients"]["lite"] = {
                "status": "unhealthy",
                "error": str(e)
            }
            if health_status["overall_status"] == "healthy":
                health_status["overall_status"] = "degraded"
        
        # Overall status logic
        if (health_status["clients"].get("pro", {}).get("status") == "unhealthy" and 
            health_status["clients"].get("lite", {}).get("status") == "unhealthy"):
            health_status["overall_status"] = "unhealthy"
        
        return health_status
    
    def reset_stats(self):
        """Reset statistics for both clients"""
        self.pro_client.request_count = 0
        self.pro_client.error_count = 0
        self.lite_client.request_count = 0
        self.lite_client.error_count = 0
        
        logger.info("Client statistics reset")

# Global instance for easy access
_llm_clients_instance = None

def get_llm_clients() -> LLMClients:
    """
    Get global LLM clients instance (singleton pattern)
    
    Returns:
        LLMClients: Global instance
    """
    global _llm_clients_instance
    
    if _llm_clients_instance is None:
        _llm_clients_instance = LLMClients()
    
    return _llm_clients_instance

def reset_llm_clients():
    """Reset the global LLM clients instance"""
    global _llm_clients_instance
    _llm_clients_instance = None

# Convenience functions for direct access
def get_pro_client() -> GeminiClient:
    """Get Pro model client directly"""
    return get_llm_clients().get_pro_client()

def get_lite_client() -> GeminiClient:
    """Get Lite model client directly"""
    return get_llm_clients().get_lite_client()

================
File: orchestrator.py
================
# Updated orchestrator.py - Adaptive Interview System

"""
Updated Orchestrator with adaptive question generation based on candidate responses
"""

import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import json

from llm_clients import LLMClients
from agents import ReviewerAgent, RecruiterAgent, InterviewerAgent
from agents.quecraft_agent import QueCraftAgent
from utils.mock_data_generator import MockDataGenerator
from utils.web_search import WebSearchTool
from utils.report_generator import ReportGenerator
from utils.tool_logger import tool_logger_instance
from config import Config

logger = logging.getLogger(__name__)

class InterviewOrchestrator:
    """
    Adaptive orchestrator that generates questions dynamically based on candidate responses
    """
    
    def __init__(self):
        """Initialize the adaptive orchestrator"""
        try:
            logger.info("Initializing Adaptive Interview Orchestrator for Excel assessment")
            
            # Initialize LLM clients
            self.llm_clients = LLMClients()
            
            # Initialize enhanced tools
            self._enhanced_data_generator = MockDataGenerator(
                llm_client=self.llm_clients.get_pro_client()
            )
            self.web_search_tool = WebSearchTool()
            self.report_generator = ReportGenerator()
            
            # Initialize enhanced agents
            self._initialize_enhanced_agents()
            
            # Adaptive interview state
            self.interview_state = "not_started"
            self.current_questions = []  # Current batch of questions (max 2)
            self.current_question_index = 0
            self.chat_history = []
            self.assessment_results = []
            self.candidate_data = {}
            self.interview_metadata = {}
            self.is_awaiting_follow_up = False
            
            # Adaptive tracking
            self.excel_skills_tested = set()
            self.excel_performance_by_skill = {}
            self.candidate_profile = {
                "strengths": [],
                "weaknesses": [],
                "skill_trajectory": [],
                "preferred_difficulty": "Medium",
                "areas_needing_focus": []
            }
            self.trajectory_decisions = []
            
            # Session tracking
            self.session_id = f"adaptive_excel_interview_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.start_time = None
            self.end_time = None
            
            logger.info("Adaptive Excel Interview Orchestrator initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize adaptive orchestrator: {str(e)}")
            raise
    
    def _initialize_enhanced_agents(self):
        """Initialize enhanced agents with proper data integration"""
        try:
            # Enhanced QueCraft Agent with adaptive capabilities
            self.quecraft_agent = QueCraftAgent(
                self.llm_clients.get_pro_client(),
                self._enhanced_data_generator,
                self.web_search_tool
            )
            
            # Reviewer Agent
            self.reviewer_agent = ReviewerAgent(
                self.llm_clients.get_pro_client()
            )
            self.reviewer_agent.add_tool("search_excel_practices", self.web_search_tool.search_excel_best_practices)
            
            # Recruiter Agent  
            self.recruiter_agent = RecruiterAgent(
                self.llm_clients.get_pro_client()
            )
            
            # Interviewer Agent
            self.interviewer_agent = InterviewerAgent(
                self.llm_clients.get_lite_client()
            )
            
            logger.info("All adaptive agents initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize adaptive agents: {str(e)}")
            raise
    
    def start_interview(self, 
                       candidate_name: str, 
                       role_type: str = "Business Analyst",
                       difficulty_level: str = "Mixed Assessment", 
                       excel_focus_areas: List[str] = None,
                       max_questions: int = 10,
                       include_datasets: bool = True) -> Dict[str, Any]:
        """
        Start adaptive Excel interview with initial question generation
        """
        try:
            logger.info(f"Starting adaptive Excel interview for {candidate_name}")
            
            # Set up candidate data
            self.candidate_data = {
                "name": candidate_name,
                "role_type": role_type,
                "difficulty_level": difficulty_level,
                "excel_focus_areas": excel_focus_areas or [],
                "max_questions": max_questions,
                "include_datasets": include_datasets
            }
            
            # Initialize candidate profile
            self.candidate_profile.update({
                "role_context": role_type,
                "initial_difficulty": difficulty_level,
                "focus_areas": excel_focus_areas or [],
                "preferred_difficulty": "Medium"  # Start with medium
            })
            
            # Enhanced interview metadata
            self.interview_metadata = {
                "session_id": self.session_id,
                "candidate": candidate_name,
                "role_context": role_type,
                "assessment_type": "adaptive_excel_methodology",
                "start_time": datetime.now().isoformat(),
                "max_questions": max_questions,
                "data_generation_enabled": include_datasets,
                "adaptive_features": True
            }
            
            self.start_time = datetime.now()
            
            # Generate initial 2 questions based on profile
            logger.info("Generating initial adaptive questions...")
            initial_questions = self._generate_next_questions(
                context="initial_assessment",
                count=2
            )
            
            if not initial_questions["success"]:
                return {
                    "success": False,
                    "error": f"Failed to generate initial questions: {initial_questions.get('error')}"
                }
            
            self.current_questions = initial_questions["questions"]
            
            # Start interview
            welcome_result = self.interviewer_agent.start_interview(candidate_name)
            if not welcome_result["success"]:
                return {"success": False, "error": f"Failed to start interview: {welcome_result.get('error')}"}
            
            # Present first question
            first_question_result = self.interviewer_agent.present_question(
                self.current_questions[0], 1, "?"  # Don't show total as it's adaptive
            )
            
            if not first_question_result["success"]:
                return {"success": False, "error": f"Failed to present first question: {first_question_result.get('error')}"}
            
            # Update state
            self.interview_state = "in_progress"
            self.current_question_index = 0
            
            # Track skill being tested
            first_skill = self.current_questions[0].get("skill_target", "Unknown")
            self.excel_skills_tested.add(first_skill)
            
            # Chat history
            self.chat_history.append({
                "timestamp": datetime.now().isoformat(),
                "type": "adaptive_excel_interview_start",
                "content": welcome_result["welcome_message"],
                "metadata": {"assessment_type": "adaptive_excel_methodology"}
            })
            
            first_question_chat = {
                "timestamp": datetime.now().isoformat(),
                "type": "adaptive_question",
                "question_id": self.current_questions[0]["question_id"],
                "content": first_question_result["question_presentation"],
                "question_data": self.current_questions[0]
            }
            
            # Include dataset info if available
            if self.current_questions[0].get("dataset_info", {}).get("generated_successfully"):
                first_question_chat["dataset_available"] = True
                first_question_chat["dataset_preview"] = self.current_questions[0]["dataset_info"].get("metadata", {})
            
            self.chat_history.append(first_question_chat)
            
            logger.info("Adaptive Excel interview started successfully")
            
            return {
                "success": True,
                "welcome_message": f"Welcome to your adaptive Excel skills assessment, {candidate_name}! This evaluation will adjust to your responses and focus on areas that need the most attention.",
                "first_question": self.current_questions[0],
                "question_presentation": first_question_result["question_presentation"],
                "interview_metadata": self.interview_metadata,
                "adaptive_mode": True,
                "questions_prepared": len(self.current_questions)
            }
            
        except Exception as e:
            logger.error(f"Error starting adaptive Excel interview: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _generate_next_questions(self, 
                                context: str = "continuation",
                                count: int = 2,
                                response_analysis: Dict = None) -> Dict[str, Any]:
        """
        Generate next batch of questions based on current context and candidate profile
        """
        try:
            # Prepare context for question generation
            generation_context = {
                "candidate_profile": self.candidate_profile,
                "assessment_results": self.assessment_results[-3:] if self.assessment_results else [],  # Last 3 results
                "skills_tested": list(self.excel_skills_tested),
                "response_analysis": response_analysis or {},
                "generation_context": context,
                "role_context": self.candidate_data["role_type"],
                "adaptive_mode": True
            }
            
            # Create adaptive prompt for question generation
            adaptive_prompt = self._create_adaptive_generation_prompt(generation_context, count)
            
            # Generate questions using QueCraft agent
            response = self.quecraft_agent.generate_response(
                user_input=adaptive_prompt,
                context=generation_context
            )
            
            if not response["success"]:
                return {"success": False, "error": "Failed to generate adaptive questions"}
            
            # Parse response
            json_result = self.quecraft_agent.parse_json_response(response["response"])
            if not json_result["success"]:
                return {"success": False, "error": "Failed to parse question generation response"}
            
            questions = json_result["data"].get("questions", [])
            
            # Enhance questions with datasets if needed
            enhanced_questions = []
            for question in questions:
                enhanced_question = self._enhance_question_with_data(question)
                enhanced_questions.append(enhanced_question)
            
            return {
                "success": True,
                "questions": enhanced_questions,
                "generation_reasoning": json_result["data"].get("reasoning", ""),
                "trajectory_decision": json_result["data"].get("trajectory_decision", "")
            }
            
        except Exception as e:
            logger.error(f"Error generating next questions: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _create_adaptive_generation_prompt(self, context: Dict, count: int) -> str:
        """Create adaptive prompt for question generation"""
        
        prompt = f"""Generate {count} adaptive Excel interview questions based on the candidate's performance profile and responses.

CANDIDATE PROFILE:
{json.dumps(context["candidate_profile"], indent=2)}

RECENT ASSESSMENT RESULTS:
{json.dumps(context["assessment_results"], indent=2)}

SKILLS ALREADY TESTED:
{context["skills_tested"]}

GENERATION CONTEXT: {context["generation_context"]}

ADAPTIVE REQUIREMENTS:
1. Analyze the candidate's performance trajectory
2. Identify skill gaps and strengths from previous responses
3. Generate questions that target areas needing improvement
4. Adjust difficulty based on demonstrated capability
5. Ensure logical progression and skill building

RESPONSE FORMAT:
{{
    "reasoning": "Explain why these specific questions were chosen based on candidate profile",
    "trajectory_decision": "Describe the adaptive trajectory being taken",
    "questions": [
        {{
            "question_id": "generated_id",
            "question": "How would you... [adaptive scenario]",
            "skill_target": "Primary Excel skill",
            "difficulty": "Easy/Medium/Hard based on candidate ability",
            "adaptive_reasoning": "Why this question fits the candidate's current level",
            "scenario_context": "Business scenario description",
            "requires_dataset": true/false,
            "dataset_requirements": {{
                "data_context": "type of data needed",
                "required_columns": ["col1", "col2"],
                "data_challenges": ["challenge1", "challenge2"],
                "sample_size": 30-100
            }},
            "expected_approach": "Key Excel methodology elements",
            "evaluation_criteria": ["criterion1", "criterion2"],
            "builds_on_previous": "How this connects to previous questions"
        }}
    ],
    "next_focus_areas": ["areas to explore in future questions"]
}}

ADAPTIVE STRATEGY:
- If candidate is performing well: Increase complexity, introduce advanced concepts
- If candidate is struggling: Reinforce fundamentals, provide supportive scenarios
- If candidate shows specific weaknesses: Target those areas with focused questions
- If candidate shows expertise in an area: Move to different skill domains

Generate questions that will help determine the candidate's true Excel capability level."""

        return prompt
    
    def _enhance_question_with_data(self, question: Dict) -> Dict:
        """Enhance a single question with contextual dataset if needed"""
        if not question.get("requires_dataset"):
            return question
        
        try:
            dataset_result = self._enhanced_data_generator.generate_contextual_dataset(
                question_context=question.get("question", ""),
                size=question.get("dataset_requirements", {}).get("sample_size", 40),
                specific_requirements=question.get("dataset_requirements", {})
            )
            
            if dataset_result.get("success"):
                question["dataset_info"] = {
                    "html_table": dataset_result["dataset_html"],
                    "csv_data": dataset_result["dataset_csv"],
                    "metadata": dataset_result["dataset_info"],
                    "context_analysis": dataset_result.get("context_analysis", {}),
                    "generated_successfully": True
                }
            else:
                question["dataset_info"] = {
                    "error": dataset_result.get("error", "Unknown error"),
                    "generated_successfully": False
                }
                
        except Exception as e:
            logger.error(f"Error enhancing question with data: {str(e)}")
            question["dataset_info"] = {
                "error": str(e),
                "generated_successfully": False
            }
        
        return question
    
    def process_response(self, candidate_answer: str) -> Dict[str, Any]:
        """
        Process candidate response and adaptively generate next question
        """
        try:
            if self.interview_state != "in_progress":
                return {"success": False, "error": "Interview not in progress"}
            
            if self.current_question_index >= len(self.current_questions):
                return {"success": False, "error": "No current question available"}
            
            current_question = self.current_questions[self.current_question_index]
            
            logger.info(f"Processing adaptive response for question {self.current_question_index + 1}")
            
            # Evaluate current response
            evaluation_context = {
                "timestamp": datetime.now().isoformat(),
                "assessment_type": "adaptive_excel_methodology",
                "response_type": "verbal_explanation",
                "dataset_available": current_question.get("dataset_info", {}).get("generated_successfully", False),
                "adaptive_context": True
            }
            
            # Log response
            self.chat_history.append({
                "timestamp": datetime.now().isoformat(),
                "type": "adaptive_response",
                "question_id": current_question["question_id"],
                "content": candidate_answer,
                "skill_target": current_question.get("skill_target"),
                "had_dataset": current_question.get("requires_dataset", False)
            })
            
            # Evaluate response
            evaluation_result = self.reviewer_agent.evaluate_response(
                current_question,
                candidate_answer,
                evaluation_context
            )
            
            if not evaluation_result["success"]:
                return {"success": False, "error": f"Failed to evaluate response: {evaluation_result.get('error')}"}
            
            evaluation_data = evaluation_result["evaluation"]
            
            # Update candidate profile based on response
            self._update_candidate_profile(current_question, evaluation_data, candidate_answer)
            
            # Create assessment record
            assessment_record = {
                "question_index": len(self.assessment_results),
                "question": current_question,
                "candidate_answer": candidate_answer,
                "evaluation": evaluation_data,
                "timestamp": datetime.now().isoformat(),
                "excel_skill": current_question.get("skill_target"),
                "adaptive_generation": True
            }
            
            self.assessment_results.append(assessment_record)
            
            # Handle follow-ups
            if evaluation_data.get("follow_up_needed"):
                follow_up_suggestion = evaluation_data.get("follow_up_suggestion", "Can you elaborate?")
                follow_up_result = self.interviewer_agent.ask_clarification(
                    clarification_type="adaptive_probe", 
                    specific_area=follow_up_suggestion
                )
                
                return {
                    "success": True,
                    "acknowledgment": evaluation_data.get("justification"),
                    "next_question": {"question": follow_up_result["clarification_question"], "is_follow_up": True},
                    "interview_complete": False,
                    "adaptive_mode": True
                }
            
            # Get acknowledgment
            acknowledgment_result = self.interviewer_agent.acknowledge_answer(
                candidate_answer,
                evaluation_data.get("grade")
            )
            acknowledgment = acknowledgment_result.get("acknowledgment", "Thank you for your response.")
            
            # Adaptive decision: should we continue?
            continue_decision = self._should_continue_adaptive_assessment()
            
            if not continue_decision["continue"]:
                return self._complete_adaptive_assessment(acknowledgment, continue_decision.get("reason"))
            
            # Move to next question or generate new ones
            self.current_question_index += 1
            
            # Check if we need to generate more questions
            if self.current_question_index >= len(self.current_questions):
                # Analyze performance and generate next batch
                response_analysis = {
                    "latest_evaluation": evaluation_data,
                    "performance_trend": self._analyze_performance_trend(),
                    "skill_gaps": self._identify_skill_gaps()
                }
                
                next_questions = self._generate_next_questions(
                    context="adaptive_continuation",
                    count=2,
                    response_analysis=response_analysis
                )
                
                if next_questions["success"]:
                    self.current_questions.extend(next_questions["questions"])
                    
                    # Log trajectory decision
                    self.trajectory_decisions.append({
                        "timestamp": datetime.now().isoformat(),
                        "reasoning": next_questions.get("generation_reasoning"),
                        "trajectory": next_questions.get("trajectory_decision"),
                        "questions_generated": len(next_questions["questions"])
                    })
                else:
                    return self._complete_adaptive_assessment(acknowledgment, "Failed to generate adaptive questions")
            
            # Present next question
            next_question = self.current_questions[self.current_question_index]
            next_question_result = self.interviewer_agent.present_question(
                next_question,
                len(self.assessment_results) + 1,
                "Adaptive"
            )
            
            if not next_question_result["success"]:
                return {"success": False, "error": f"Failed to present next question: {next_question_result.get('error')}"}
            
            # Track new skill
            next_skill = next_question.get("skill_target", "Unknown")
            self.excel_skills_tested.add(next_skill)
            
            # Chat log for next question
            next_question_chat = {
                "timestamp": datetime.now().isoformat(),
                "type": "adaptive_question",
                "question_id": next_question["question_id"],
                "content": next_question_result["question_presentation"],
                "question_data": next_question,
                "adaptive_reasoning": next_question.get("adaptive_reasoning", "")
            }
            
            if next_question.get("dataset_info", {}).get("generated_successfully"):
                next_question_chat["dataset_available"] = True
            
            self.chat_history.append(next_question_chat)
            
            response_data = {
                "success": True,
                "acknowledgment": acknowledgment,
                "next_question": next_question,
                "question_presentation": next_question_result["question_presentation"],
                "evaluation": evaluation_data,
                "interview_complete": False,
                "adaptive_mode": True,
                "candidate_profile_updated": True
            }
            
            # Include dataset info if available
            if next_question.get("dataset_info", {}).get("generated_successfully"):
                response_data["next_question_has_dataset"] = True
                response_data["dataset_info"] = {
                    "html_table": next_question["dataset_info"].get("html_table", ""),
                    "metadata": next_question["dataset_info"].get("metadata", {})
                }
            
            return response_data
            
        except Exception as e:
            logger.error(f"Error processing adaptive response: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _update_candidate_profile(self, question: Dict, evaluation: Dict, answer: str):
        """Update candidate profile based on latest response"""
        skill = question.get("skill_target", "Unknown")
        score = evaluation.get("score", 0)
        grade = evaluation.get("grade", "")
        
        # Update skill performance tracking
        if skill not in self.excel_performance_by_skill:
            self.excel_performance_by_skill[skill] = []
        
        self.excel_performance_by_skill[skill].append({
            "score": score,
            "grade": grade,
            "question_difficulty": question.get("difficulty", ""),
            "timestamp": datetime.now().isoformat()
        })
        
        # Update candidate profile
        self.candidate_profile["skill_trajectory"].append({
            "skill": skill,
            "score": score,
            "difficulty": question.get("difficulty", ""),
            "timestamp": datetime.now().isoformat()
        })
        
        # Update strengths and weaknesses
        if score >= 80:
            if skill not in self.candidate_profile["strengths"]:
                self.candidate_profile["strengths"].append(skill)
        elif score < 60:
            if skill not in self.candidate_profile["weaknesses"]:
                self.candidate_profile["weaknesses"].append(skill)
                
            if skill not in self.candidate_profile["areas_needing_focus"]:
                self.candidate_profile["areas_needing_focus"].append(skill)
        
        # Adjust preferred difficulty
        recent_scores = [item["score"] for item in self.candidate_profile["skill_trajectory"][-3:]]
        if recent_scores:
            avg_recent = sum(recent_scores) / len(recent_scores)
            if avg_recent >= 85:
                self.candidate_profile["preferred_difficulty"] = "Hard"
            elif avg_recent >= 70:
                self.candidate_profile["preferred_difficulty"] = "Medium"
            else:
                self.candidate_profile["preferred_difficulty"] = "Easy"
    
    def _analyze_performance_trend(self) -> Dict[str, Any]:
        """Analyze candidate's performance trend"""
        if len(self.candidate_profile["skill_trajectory"]) < 2:
            return {"trend": "insufficient_data"}
        
        recent_scores = [item["score"] for item in self.candidate_profile["skill_trajectory"][-3:]]
        
        if len(recent_scores) >= 2:
            if recent_scores[-1] > recent_scores[0] + 10:
                trend = "improving"
            elif recent_scores[-1] < recent_scores[0] - 10:
                trend = "declining"
            else:
                trend = "stable"
        else:
            trend = "stable"
        
        return {
            "trend": trend,
            "recent_average": sum(recent_scores) / len(recent_scores),
            "score_range": [min(recent_scores), max(recent_scores)]
        }
    
    def _identify_skill_gaps(self) -> List[str]:
        """Identify skills that need more attention"""
        all_excel_skills = [
            "Basic Formulas and Functions",
            "Data Manipulation and Cleaning", 
            "Lookup Functions (VLOOKUP, INDEX/MATCH)",
            "Pivot Tables and Data Analysis",
            "Data Visualization and Charts",
            "Conditional Logic and IF Statements",
            "Advanced Functions and Array Formulas"
        ]
        
        tested_skills = list(self.excel_skills_tested)
        weak_skills = self.candidate_profile["areas_needing_focus"]
        untested_skills = [skill for skill in all_excel_skills if skill not in tested_skills]
        
        # Priority: weak skills first, then untested skills
        return weak_skills + untested_skills[:2]  # Limit to avoid too many
    
    def _should_continue_adaptive_assessment(self) -> Dict[str, Any]:
        """Determine if adaptive assessment should continue"""
        
        # Minimum questions check
        if len(self.assessment_results) < 4:
            return {"continue": True, "reason": "Need minimum assessment coverage"}
        
        # Maximum questions check
        if len(self.assessment_results) >= self.candidate_data.get("max_questions", 10):
            return {"continue": False, "reason": "Reached maximum question limit"}
        
        # Performance stability check
        if len(self.assessment_results) >= 6:
            recent_scores = [r["evaluation"]["score"] for r in self.assessment_results[-4:]]
            score_variance = max(recent_scores) - min(recent_scores)
            
            if score_variance < 15:  # Scores are stable
                return {"continue": False, "reason": "Performance level established"}
        
        # Skill coverage check
        target_skills = 5  # Target number of different skills to assess
        if len(self.excel_skills_tested) >= target_skills:
            return {"continue": False, "reason": "Adequate skill coverage achieved"}
        
        return {"continue": True, "reason": "Continue adaptive assessment"}
    
    def _complete_adaptive_assessment(self, acknowledgment: str, reason: str) -> Dict[str, Any]:
        """Complete the adaptive assessment"""
        try:
            logger.info(f"Completing adaptive assessment: {reason}")
            
            self.interview_state = "completed"
            self.end_time = datetime.now()
            
            # Enhanced context for final decision
            adaptive_context = {
                "assessment_type": "adaptive_excel_methodology",
                "trajectory_decisions": self.trajectory_decisions,
                "candidate_profile": self.candidate_profile,
                "adaptive_features_used": True
            }
            
            final_decision_result = self.recruiter_agent.make_final_decision(
                self.interview_metadata,
                self.assessment_results,
                adaptive_context
            )
            
            if not final_decision_result["success"]:
                final_decision = {
                    "decision": "Requires Manual Review",
                    "overall_score": 0,
                    "recommendation_summary": "Unable to generate adaptive assessment decision."
                }
            else:
                final_decision = final_decision_result["final_decision"]
            
            # Get conclusion
            conclusion_result = self.interviewer_agent.conclude_interview(
                final_decision.get("decision", "Complete"),
                "Thank you for participating in this adaptive Excel skills assessment."
            )
            
            conclusion_message = conclusion_result.get("conclusion_message", 
                "Thank you for completing the adaptive Excel assessment!")
            
            # Update metadata
            self.interview_metadata.update({
                "end_time": datetime.now().isoformat(),
                "duration": str(self.end_time - self.start_time),
                "questions_completed": len(self.assessment_results),
                "final_decision": final_decision,
                "adaptive_trajectory_decisions": len(self.trajectory_decisions),
                "candidate_profile_final": self.candidate_profile
            })
            
            return {
                "success": True,
                "acknowledgment": acknowledgment,
                "conclusion_message": conclusion_message,
                "interview_complete": True,
                "final_decision": final_decision,
                "adaptive_summary": {
                    "trajectory_decisions": len(self.trajectory_decisions),
                    "skills_discovered": list(self.excel_skills_tested),
                    "performance_trend": self._analyze_performance_trend(),
                    "adaptive_personalization": True
                }
            }
            
        except Exception as e:
            logger.error(f"Error completing adaptive assessment: {str(e)}")
            return {"success": False, "error": str(e), "interview_complete": True}
    
    def get_current_question_dataset(self) -> Dict[str, Any]:
        """Get dataset for current question"""
        try:
            if (self.interview_state != "in_progress" or 
                self.current_question_index >= len(self.current_questions)):
                return {"success": False, "error": "No current question available"}
            
            current_question = self.current_questions[self.current_question_index]
            dataset_info = current_question.get("dataset_info", {})
            
            if not dataset_info.get("generated_successfully"):
                return {"success": False, "error": "No dataset available for current question"}
            
            return {
                "success": True,
                "dataset_html": dataset_info.get("html_table", ""),
                "dataset_metadata": dataset_info.get("metadata", {}),
                "context_analysis": dataset_info.get("context_analysis", {}),
                "question_id": current_question.get("question_id"),
                "skill_target": current_question.get("skill_target")
            }
            
        except Exception as e:
            logger.error(f"Error getting current question dataset: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def get_progress(self) -> Dict[str, Any]:
        """Get adaptive assessment progress"""
        return {
            "questions_completed": len(self.assessment_results),
            "current_question_number": len(self.assessment_results) + 1,
            "interview_state": self.interview_state,
            "skills_tested": list(self.excel_skills_tested),
            "adaptive_mode": True,
            "trajectory_decisions": len(self.trajectory_decisions),
            "candidate_profile": {
                "strengths": self.candidate_profile["strengths"],
                "areas_needing_focus": self.candidate_profile["areas_needing_focus"],
                "preferred_difficulty": self.candidate_profile["preferred_difficulty"]
            },
            "performance_trend": self._analyze_performance_trend().get("trend", "unknown")
        }
    
    def get_final_results(self) -> Dict[str, Any]:
        """Get comprehensive adaptive assessment results"""
        try:
            if self.interview_state != "completed":
                return {"success": False, "error": "Adaptive assessment not completed"}
            
            performance_metrics = self._calculate_adaptive_performance_metrics()
            
            return {
                "success": True,
                "final_decision": self.interview_metadata.get("final_decision", {}),
                "performance_metrics": performance_metrics,
                "interview_metadata": self.interview_metadata,
                "assessment_results": self.assessment_results,
                "excel_skills_performance": self.excel_performance_by_skill,
                "candidate_profile": self.candidate_profile,
                "adaptive_features": {
                    "trajectory_decisions": self.trajectory_decisions,
                    "adaptive_personalization": True,
                    "dynamic_question_generation": True,
                    "questions_adapted": len(self.trajectory_decisions),
                    "skills_discovered": len(self.excel_skills_tested),
                    "final_difficulty_level": self.candidate_profile.get("preferred_difficulty", "Medium"),
                    "performance_trend": self._analyze_performance_trend().get("trend", "stable")
                }
            }
        except Exception as e:
            logger.error(f"Error getting adaptive assessment results: {str(e)}")
            return {"success": False, "error": str(e)}

================
File: README.md
================
# Smart-Sheet
An AI-powered, response-driven interview system that dynamically evaluates Microsoft Excel proficiency through case-based scenarios and adaptive questioning.

## Overview

This system revolutionizes Excel skills assessment by generating questions in real-time based on candidate responses, creating personalized learning paths and providing contextually relevant datasets for each scenario. Unlike traditional fixed-sequence assessments, this platform adapts to individual performance levels and focuses on areas that need the most attention.

## Key Features

### Adaptive Question Generation
- **Dynamic Question Creation**: Questions are generated in batches of 2 based on candidate performance
- **Response-Driven Trajectory**: Assessment path changes based on demonstrated capabilities
- **Real-Time Adaptation**: System adjusts difficulty and focus areas based on ongoing evaluation
- **Intelligent Progression**: Each question builds on previous responses and identified skill gaps

### Contextual Data Generation
- **Business-Realistic Datasets**: AI generates datasets that perfectly match question scenarios
- **Data Quality Challenges**: Includes realistic data issues (missing values, inconsistencies) for practice
- **Context-Aware Creation**: Datasets align with specific Excel challenges being tested
- **HTML Table Display**: Clean, professional presentation of generated data

### Performance Analysis
- **Candidate Profile Evolution**: Tracks strengths, weaknesses, and learning patterns
- **Trend Analysis**: Monitors performance improvement, decline, or stability
- **Skill Gap Identification**: Automatically identifies untested or weak skill areas
- **Difficulty Calibration**: Adjusts question complexity based on demonstrated ability

### Case-Based Assessment
- **Scenario-Driven Questions**: Realistic business situations requiring Excel solutions
- **Methodology Focus**: Tests Excel thinking and problem-solving approach
- **No Software Required**: Candidates explain their approach verbally
- **Progressive Complexity**: Questions advance based on individual capability

## System Architecture

### Core Components

#### 1. Adaptive Orchestrator (`AdaptiveInterviewOrchestrator`)
- **Purpose**: Main controller managing adaptive interview flow
- **Responsibilities**:
  - Candidate profile management
  - Dynamic question generation coordination
  - Performance trend analysis
  - Trajectory decision making
  - Assessment completion logic

#### 2. AI Agents

##### QueCraft Agent
- **Function**: Case-based question generation with contextual data
- **Capabilities**:
  - Scenario design based on business contexts
  - Dataset requirement specification
  - Question-data alignment validation
  - Adaptive reasoning integration

##### Reviewer Agent
- **Function**: Response evaluation and scoring
- **Capabilities**:
  - Excel methodology assessment
  - Problem-solving logic evaluation
  - Strengths and weaknesses identification
  - Follow-up question recommendations

##### Recruiter Agent
- **Function**: Final decision making and comprehensive assessment
- **Capabilities**:
  - Overall performance synthesis
  - Hiring recommendation generation
  - Skill competency rating
  - Development area identification

##### Interviewer Agent
- **Function**: Conversational interface management
- **Capabilities**:
  - Question presentation
  - Response acknowledgment
  - Clarification requests
  - Interview flow management

#### 3. Enhanced Data Generator (`MockDataGenerator`)
- **LLM Integration**: Uses AI to understand question context
- **Smart Data Creation**: Generates datasets that support specific Excel challenges
- **Quality Control**: Includes realistic data issues for comprehensive testing
- **Format Flexibility**: Outputs data in multiple formats (HTML, CSV, JSON)

### Technical Infrastructure

#### LLM Client Management
- **Dual Model Architecture**: 
  - Gemini Pro for complex reasoning tasks
  - Gemini Flash for conversational interactions
- **Rate Limiting**: Intelligent request management
- **Error Handling**: Comprehensive failure recovery
- **Performance Monitoring**: Real-time usage tracking

#### Tool Integration
- **Web Search**: Research Excel best practices and current information
- **Report Generation**: Comprehensive feedback and analysis reports
- **Tool Logging**: Detailed performance and usage analytics
- **JSON Processing**: Robust parsing and validation

## Assessment Process

### 1. Initialization Phase
```
Candidate Setup → Profile Creation → Initial Question Generation (2 questions)
```

### 2. Adaptive Assessment Loop
```
Present Question → Collect Response → Evaluate Performance → Update Profile → 
Generate Next Questions (if needed) → Continue or Complete
```

### 3. Decision Points
- **Continue Assessment**: Based on skill coverage and performance stability
- **Adjust Difficulty**: Increase/decrease complexity based on demonstrated ability
- **Focus Areas**: Target weak skills or explore untested domains
- **Complete Assessment**: When sufficient data collected or maximum questions reached

### 4. Results Generation
```
Performance Analysis → Skill Assessment → Trajectory Review → 
Personalized Feedback → Comprehensive Report
```

## Detailed Features

### Adaptive Intelligence
- **Profile Learning**: System builds detailed candidate profile during assessment
- **Trajectory Decisions**: Logged reasoning for each adaptive choice made
- **Performance Prediction**: Anticipates candidate capability in untested areas
- **Personalization Engine**: Tailors experience to individual learning patterns

### Excel Skills Coverage
- Basic Formulas and Functions
- Data Manipulation and Cleaning
- Lookup Functions (VLOOKUP, INDEX/MATCH)
- Pivot Tables and Data Analysis
- Data Visualization and Charts
- Conditional Logic and IF Statements
- Text Functions and String Manipulation
- Advanced Functions and Array Formulas
- Data Validation and Error Handling
- Macro Basics and Automation Concepts

### Business Scenarios
- Marketing Campaign Analysis
- Sales Performance Tracking
- Financial Reporting and Budgeting
- Inventory Management
- Employee Performance Metrics
- Customer Data Analysis
- Project Timeline Management
- Survey Data Processing

### Assessment Metrics
- **Overall Proficiency Score**: Comprehensive performance rating
- **Skill-Specific Assessment**: Individual area competency levels
- **Performance Trend**: Learning trajectory analysis
- **Adaptive Effectiveness**: Personalization success measurement
- **Question Efficiency**: Optimal assessment coverage analysis

## Technical Requirements

### Dependencies
```
streamlit>=1.29.0
google-generativeai>=0.3.0
pandas>=2.0.0
python-dotenv>=1.0.0
requests>=2.31.0
faker>=20.0.0
logging>=0.4.9.6
datetime
json5>=0.9.0
typing-extensions>=4.8.0
faker>=20.0.0
google-genai>=1.38.0
```

### Environment Setup
1. Google API Key for Gemini models
2. Python 3.8+ environment
3. Streamlit web interface
4. Required Python packages

### Configuration
- **Model Selection**: Gemini 2.0 Flash for optimal performance
- **Rate Limiting**: Configurable API request limits
- **Assessment Parameters**: Customizable question counts and difficulty ranges
- **Data Generation**: Adjustable dataset sizes and complexity

## User Interface

### Streamlit Web Application
- **Responsive Design**: Works across different screen sizes
- **Professional Styling**: Clean, corporate-appropriate interface
- **Real-Time Updates**: Dynamic progress tracking and feedback
- **Interactive Elements**: Dataset viewers, progress indicators, performance charts

### Key UI Components
- **Candidate Setup**: Profile configuration and preferences
- **Chat Interface**: Conversational assessment experience
- **Dataset Viewer**: Contextual data exploration
- **Progress Tracking**: Real-time adaptive decisions and skill coverage
- **Results Dashboard**: Comprehensive performance analysis
- **Report Generation**: Downloadable feedback documents

## Deployment

### Local Development
```bash
# Clone repository
git clone [https://github.com/adityalakhani/Smart-Sheet](https://github.com/adityalakhani/Smart-Sheet)

# Install dependencies
pip install -r requirements.txt

# Set environment variables
export GOOGLE_API_KEY=your_api_key

# Run application
streamlit run app.py
```

### Production Considerations
- **Scalability**: Horizontal scaling for multiple concurrent assessments
- **Security**: API key management and data protection
- **Monitoring**: Performance tracking and error logging
- **Backup**: Assessment data and configuration backup strategies

## Data Privacy and Security

### Data Handling
- **Minimal Storage**: Only assessment-relevant data retained
- **Secure Processing**: API communications encrypted
- **Privacy Compliance**: Follows data protection best practices
- **Temporary Data**: Generated datasets exist only during assessment

### Security Features
- **API Key Protection**: Environment variable configuration
- **Rate Limiting**: Prevents abuse and manages costs
- **Error Handling**: Graceful failure without data exposure
- **Audit Logging**: Comprehensive activity tracking

## Performance Optimization

### System Efficiency
- **Smart Caching**: Reduces redundant API calls
- **Batch Processing**: Optimizes question generation
- **Lazy Loading**: Datasets generated only when needed
- **Resource Management**: Efficient memory and API usage

### Assessment Optimization
- **Targeted Questioning**: Focuses on areas needing evaluation
- **Adaptive Stopping**: Completes when sufficient data collected
- **Skill Prioritization**: Tests critical areas first
- **Efficiency Metrics**: Tracks assessment effectiveness

## Future Enhancements

### Planned Features
- **Multi-Language Support**: Assessment in different languages
- **Advanced Analytics**: Machine learning-based insights
- **Integration APIs**: Connect with HR systems and LMS platforms
- **Mobile Optimization**: Enhanced mobile assessment experience
- **Collaborative Features**: Team assessment capabilities

### Technical Improvements
- **Performance Optimization**: Further response time improvements
- **Advanced AI Models**: Integration with newer language models
- **Enhanced Data Generation**: More sophisticated dataset creation
- **Expanded Skill Coverage**: Additional Microsoft Office applications

## Contributing

### Development Setup
1. Fork the repository
2. Create feature branch
3. Implement changes with tests
4. Submit pull request with detailed description

### Code Standards
- **PEP 8**: Python code formatting
- **Type Hints**: Comprehensive type annotations
- **Documentation**: Detailed docstrings and comments
- **Testing**: Unit tests for core functionality

## License

This project is licensed under the MIT License. See LICENSE file for details.

## Support

For technical support, bug reports, or feature requests, please create an issue in the project repository with detailed information about your environment and the specific problem encountered.

## Acknowledgments

This project leverages several open-source libraries and tools:
- **Streamlit**: Web application framework
- **Google Gemini**: Large language model API
- **Pandas**: Data manipulation and analysis
- **Faker**: Realistic test data generation

---

Made with ❤ by Aditya Lakhani

================
File: requirements.txt
================
streamlit>=1.29.0
google-generativeai>=0.3.0
pandas>=2.0.0
python-dotenv>=1.0.0
requests>=2.31.0
faker>=20.0.0
logging>=0.4.9.6
datetime
json5>=0.9.0
typing-extensions>=4.8.0
faker>=20.0.0
google-genai>=1.38.0

================
File: agents\base_agent.py
================
"""
Base Agent class that all other agents inherit from
"""

import json
import logging
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional
from utils.json_parser import JSONParser

logger = logging.getLogger(__name__)

class BaseAgent(ABC):
    """
    Abstract base class for all AI agents in the interview system
    """
    
    def __init__(self, name: str, llm_client, tools: Optional[list] = None):
        """
        Initialize the base agent
        
        Args:
            name (str): Name of the agent
            llm_client: The LLM client (Gemini) to use
            tools (list, optional): List of tools available to the agent
        """
        self.name = name
        self.llm_client = llm_client
        self.tools = tools or []
        self.conversation_history = []
        
    @abstractmethod
    def get_system_prompt(self) -> str:
        """
        Get the system prompt that defines this agent's role and behavior
        
        Returns:
            str: The system prompt for this agent
        """
        pass
    
    def add_tool(self, tool_name: str, tool_function):
        """
        Add a tool to the agent's available tools
        
        Args:
            tool_name (str): Name of the tool
            tool_function: The function to execute
        """
        self.tools.append({
            'name': tool_name,
            'function': tool_function
        })
    
    def execute_tool(self, tool_name: str, **kwargs):
        """
        Execute a specific tool by name
        
        Args:
            tool_name (str): Name of the tool to execute
            **kwargs: Arguments to pass to the tool function
            
        Returns:
            Any: Result from the tool execution
        """
        for tool in self.tools:
            if tool['name'] == tool_name:
                try:
                    return tool['function'](**kwargs)
                except Exception as e:
                    logger.error(f"Error executing tool {tool_name}: {str(e)}")
                    return None
        
        logger.warning(f"Tool {tool_name} not found")
        return None
    
    def _serialize_context_safely(self, context: Dict[str, Any]) -> str:
        """
        Safely serialize context, handling non-serializable objects
        
        Args:
            context: Context dictionary
            
        Returns:
            str: Serialized context string
        """
        def safe_serialize(obj):
            """Recursively serialize objects, converting non-serializable types"""
            if obj is None:
                return None
            elif isinstance(obj, (str, int, float, bool)):
                return obj
            elif isinstance(obj, (list, tuple)):
                return [safe_serialize(item) for item in obj]
            elif isinstance(obj, dict):
                return {key: safe_serialize(value) for key, value in obj.items()}
            else:
                # Convert non-serializable objects to string representation
                return str(obj)
        
        try:
            safe_context = safe_serialize(context)
            return json.dumps(safe_context, indent=2)
        except Exception as e:
            logger.warning(f"Context serialization failed: {e}, using string representation")
            return str(context)
    
    def generate_response(self, user_input: str, context: Optional[Dict] = None) -> Dict[str, Any]:
        """
        Generate a response using the LLM client
        
        Args:
            user_input (str): The user's input or question
            context (dict, optional): Additional context for the response
            
        Returns:
            dict: The agent's response
        """
        try:
            # Prepare the prompt with system instructions and context
            system_prompt = self.get_system_prompt()
            
            if context:
                # Use safe serialization to avoid JSON errors
                context_str = f"\nContext: {self._serialize_context_safely(context)}"
            else:
                context_str = ""
            
            full_prompt = f"{system_prompt}{context_str}\n\nUser Input: {user_input}"
            
            # Generate response using Gemini
            response = self.llm_client.generate_content(
                contents=full_prompt,
            )
            
            # Store in conversation history
            self.conversation_history.append({
                "input": user_input,
                "output": response.text,
                "context": context
            })
            
            return {
                "success": True,
                "response": response.text,
                "raw_response": response
            }
            
        except Exception as e:
            logger.error(f"Error generating response for {self.name}: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "response": None
            }
    
    def parse_json_response(self, response_text: str) -> Dict[str, Any]:
        """
        Parse JSON response from LLM output
        
        Args:
            response_text (str): Raw response text from LLM
            
        Returns:
            dict: Parsed JSON or error information
        """
        return JSONParser.parse_json_response(response_text)
    
    def reset_conversation(self):
        """Reset the conversation history"""
        self.conversation_history = []

================
File: agents\interviewer_agent.py
================
"""
Interviewer Agent - Handles conversational interface with candidates (uses lighter model)
"""

from .base_agent import BaseAgent
from utils.prompts import INTERVIEWER_SYSTEM_PROMPT

class InterviewerAgent(BaseAgent):
    """
    Interviewer Agent manages the conversational flow with candidates
    Uses the lighter Gemini model for better performance
    """
    
    def __init__(self, llm_client_lite, tools=None):
        # Note: This agent uses the lite client for faster responses
        super().__init__("Interviewer", llm_client_lite, tools)
        self.interview_state = "not_started"
        self.current_question = None
        self.candidate_responses = []
    
    def get_system_prompt(self) -> str:
        """Get the system prompt for Interviewer agent"""
        return INTERVIEWER_SYSTEM_PROMPT
    
    def generate_response(self, user_input: str, context=None) -> dict:
        """
        Override to use the lite model (gemini-2.5-flash)
        """
        try:
            system_prompt = self.get_system_prompt()
            
            if context:
                context_str = f"\nContext: {context}"
            else:
                context_str = ""
            
            full_prompt = f"{system_prompt}{context_str}\n\nUser Input: {user_input}"
            
            # Use the lighter model for interviewer responses
            response = self.llm_client.generate_content(
                contents=full_prompt,
            )
            
            self.conversation_history.append({
                "input": user_input,
                "output": response.text,
                "context": context
            })
            
            return {
                "success": True,
                "response": response.text,
                "raw_response": response
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "response": None
            }
    
    def start_interview(self, candidate_name: str = "Candidate") -> dict:
        """
        Start the interview with a welcome message
        
        Args:
            candidate_name (str): Name of the candidate
            
        Returns:
            dict: Welcome message and instructions
        """
        self.interview_state = "started"
        
        context = {
            "candidate_name": candidate_name,
            "interview_stage": "welcome",
            "action": "start_interview"
        }
        
        user_input = f"Start interview for {candidate_name}. Provide welcome message and instructions."
        
        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return {
                "success": True,
                "welcome_message": response["response"],
                "interview_state": self.interview_state
            }
        else:
            return response
    
    def present_question(self, question: dict, question_number: int, total_questions: int) -> dict:
        """
        Present a question to the candidate in a conversational manner
        
        Args:
            question (dict): Question details from interview plan
            question_number (int): Current question number
            total_questions (int): Total number of questions
            
        Returns:
            dict: Formatted question presentation
        """
        self.current_question = question
        
        context = {
            "question": question,
            "question_number": question_number,
            "total_questions": total_questions,
            "interview_stage": "presenting_question",
            "action": "present_question"
        }
        
        user_input = f"""Present question {question_number} of {total_questions} to the candidate:

Question Details:
- Question: {question.get('question', '')}
- Skill Target: {question.get('skill_target', '')}
- Difficulty: {question.get('difficulty', '')}

Make it conversational and encouraging. If the question requires a dataset, mention that clearly."""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return {
                "success": True,
                "question_presentation": response["response"],
                "current_question": question,
                "progress": f"{question_number}/{total_questions}"
            }
        else:
            return response
    
    def acknowledge_answer(self, candidate_answer: str, evaluation_grade: str = None) -> dict:
        """
        Acknowledge candidate's answer and provide transition
        
        Args:
            candidate_answer (str): The candidate's response
            evaluation_grade (str, optional): Grade from reviewer (for internal use)
            
        Returns:
            dict: Acknowledgment message
        """
        self.candidate_responses.append({
            "question": self.current_question,
            "answer": candidate_answer,
            "grade": evaluation_grade
        })
        
        context = {
            "candidate_answer": candidate_answer,
            "evaluation_grade": evaluation_grade,
            "interview_stage": "acknowledging_answer",
            "action": "acknowledge_answer"
        }
        
        user_input = f"""Acknowledge the candidate's answer and provide a smooth transition:

Candidate's Answer: {candidate_answer}

Be encouraging and professional. Don't reveal the evaluation grade. Prepare for the next question or conclusion."""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return {
                "success": True,
                "acknowledgment": response["response"],
                "recorded_response": True
            }
        else:
            return response
    
    def ask_clarification(self, clarification_type: str, specific_area: str) -> dict:
        """
        Ask for clarification on a candidate's response
        
        Args:
            clarification_type (str): Type of clarification needed
            specific_area (str): Specific area that needs clarification
            
        Returns:
            dict: Clarification question
        """
        context = {
            "clarification_type": clarification_type,
            "specific_area": specific_area,
            "current_question": self.current_question,
            "interview_stage": "seeking_clarification",
            "action": "ask_clarification"
        }
        
        user_input = f"""Ask for clarification about {specific_area}. 

Clarification type: {clarification_type}
Current question context: {self.current_question.get('question', '') if self.current_question else 'N/A'}

Be polite and specific in asking for clarification. Don't make the candidate feel bad about their initial response."""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return {
                "success": True,
                "clarification_question": response["response"],
                "awaiting_clarification": True
            }
        else:
            return response
    
    def provide_encouragement(self, performance_context: str = "general") -> dict:
        """
        Provide encouragement to the candidate during the interview
        
        Args:
            performance_context (str): Context for encouragement
            
        Returns:
            dict: Encouraging message
        """
        context = {
            "performance_context": performance_context,
            "interview_stage": "encouragement",
            "action": "provide_encouragement"
        }
        
        user_input = f"""Provide encouragement to the candidate. Context: {performance_context}

Be genuine, supportive, and maintain professional interview atmosphere."""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return {
                "success": True,
                "encouragement_message": response["response"]
            }
        else:
            return response
    
    def conclude_interview(self, interview_outcome: str, next_steps: str = "") -> dict:
        """
        Conclude the interview with appropriate closing remarks
        
        Args:
            interview_outcome (str): Overall interview outcome context
            next_steps (str): Information about next steps
            
        Returns:
            dict: Concluding message
        """
        self.interview_state = "concluded"
        
        context = {
            "interview_outcome": interview_outcome,
            "next_steps": next_steps,
            "total_responses": len(self.candidate_responses),
            "interview_stage": "conclusion",
            "action": "conclude_interview"
        }
        
        user_input = f"""Conclude the interview professionally:

Interview outcome context: {interview_outcome}
Next steps: {next_steps}
Total questions answered: {len(self.candidate_responses)}

Thank the candidate, mention next steps if provided, and maintain positive tone regardless of performance."""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return {
                "success": True,
                "conclusion_message": response["response"],
                "interview_state": self.interview_state,
                "total_responses_recorded": len(self.candidate_responses)
            }
        else:
            return response
    
    def handle_off_topic_response(self, candidate_input: str) -> dict:
        """
        Handle when candidate gives off-topic or unclear responses
        
        Args:
            candidate_input (str): The candidate's off-topic response
            
        Returns:
            dict: Redirect message
        """
        context = {
            "candidate_input": candidate_input,
            "current_question": self.current_question,
            "interview_stage": "redirecting",
            "action": "handle_off_topic"
        }
        
        user_input = f"""The candidate gave an off-topic response: {candidate_input}

Current question: {self.current_question.get('question', '') if self.current_question else 'N/A'}

Politely redirect them back to the current Excel question. Be helpful and understanding."""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return {
                "success": True,
                "redirect_message": response["response"],
                "needs_refocus": True
            }
        else:
            return response
    
    def get_interview_summary(self) -> dict:
        """
        Get summary of the interview conversation
        
        Returns:
            dict: Interview summary
        """
        return {
            "interview_state": self.interview_state,
            "total_questions_answered": len(self.candidate_responses),
            "conversation_length": len(self.conversation_history),
            "candidate_responses": self.candidate_responses,
            "current_question": self.current_question
        }
    
    def reset_interview(self):
        """Reset the interview state for a new candidate"""
        self.interview_state = "not_started"
        self.current_question = None
        self.candidate_responses = []
        self.reset_conversation()
    
    def provide_hint(self, hint_level: str = "gentle") -> dict:
        """
        Provide a hint for the current question
        
        Args:
            hint_level (str): Level of hint (gentle/moderate/direct)
            
        Returns:
            dict: Hint message
        """
        if not self.current_question:
            return {
                "success": False,
                "error": "No current question to provide hint for"
            }
        
        context = {
            "current_question": self.current_question,
            "hint_level": hint_level,
            "interview_stage": "providing_hint",
            "action": "provide_hint"
        }
        
        user_input = f"""Provide a {hint_level} hint for the current question:

Question: {self.current_question.get('question', '')}
Skill Target: {self.current_question.get('skill_target', '')}
Expected Approach: {self.current_question.get('expected_approach', '')}

Give a helpful hint without giving away the complete answer. Match the hint level requested."""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return {
                "success": True,
                "hint_message": response["response"],
                "hint_provided": True,
                "hint_level": hint_level
            }
        else:
            return response
    
    def check_understanding(self, concept: str) -> dict:
        """
        Check candidate's understanding of a specific Excel concept
        
        Args:
            concept (str): Excel concept to check understanding of
            
        Returns:
            dict: Understanding check question
        """
        context = {
            "concept": concept,
            "interview_stage": "checking_understanding",
            "action": "check_understanding"
        }
        
        user_input = f"""Ask a quick understanding check question about: {concept}

Make it conversational and not intimidating. This is to gauge their conceptual understanding."""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return {
                "success": True,
                "understanding_check": response["response"],
                "concept_being_checked": concept
            }
        else:
            return response

================
File: agents\quecraft_agent.py
================
"""
Enhanced QueCraft Agent with integrated contextual data generation
"""

import json
import logging
import time
from typing import Dict, Any, List, Optional
from agents.base_agent import BaseAgent
from utils.prompts import QUERAFT_SYSTEM_PROMPT
from utils.tool_logger import tool_logger_instance, log_tool_call

logger = logging.getLogger(__name__)

class QueCraftAgent(BaseAgent):
    """
    Enhanced QueCraft Agent for creating case-based Excel interview questions
    with contextually relevant data generation
    """
    
    def __init__(self, llm_client, enhanced_data_generator, web_search_tool=None):
        super().__init__("QueCraft", llm_client)
        self.enhanced_data_generator = enhanced_data_generator
        self.web_search_tool = web_search_tool
        
        # Add tools with logging
        self.add_tool("generate_contextual_data", self._logged_generate_contextual_data)
        self.add_tool("search_excel_practices", self._logged_web_search)
        
        logger.info("Enhanced QueCraft Agent initialized with contextual data generation")
    
    def get_system_prompt(self) -> str:
        """Get the enhanced system prompt for case-based Excel assessment"""
        return QUERAFT_SYSTEM_PROMPT
    
    @log_tool_call("generate_contextual_data", "QueCraft")
    def _logged_generate_contextual_data(self, **kwargs) -> Dict[str, Any]:
        """Generate contextual data with comprehensive logging"""
        try:
            result = self.enhanced_data_generator.generate_contextual_dataset(**kwargs)
            return result
        except Exception as e:
            logger.error(f"Error in contextual data generation: {str(e)}")
            return {"success": False, "error": str(e)}
    
    @log_tool_call("search_excel_practices", "QueCraft")  
    def _logged_web_search(self, **kwargs) -> Dict[str, Any]:
        """Perform web search with logging"""
        if self.web_search_tool:
            return self.web_search_tool.search_excel_topic(**kwargs)
        else:
            return {"success": False, "error": "Web search tool not available"}
    
    def generate_case_based_interview_plan(self, 
                                         role_context: str = "Business Analyst",
                                         difficulty_level: str = "Mixed",
                                         focus_areas: List[str] = None) -> Dict[str, Any]:
        """
        Generate comprehensive case-based interview plan with contextual data
        """
        
        logger.info(f"Generating case-based interview plan - Role: {role_context}, Difficulty: {difficulty_level}")
        
        context = {
            "role_context": role_context,
            "difficulty_level": difficulty_level, 
            "focus_areas": focus_areas or [],
            "request_type": "case_based_interview_plan"
        }
        
        user_input = f"""Generate a comprehensive case-based Excel interview plan for a {role_context} role with {difficulty_level} difficulty level.

CRITICAL REQUIREMENTS:
1. Create 8-10 CASE-BASED questions that test Excel THINKING, not software execution
2. Each question should present a realistic business scenario
3. Questions must be INTERROGATIVE ("How would you...") not directive ("Please do...")
4. Focus on methodology, approach, and Excel tool selection reasoning
5. For questions requiring datasets, specify EXACT data requirements

RESPONSE FORMAT:
{{
    "interview_plan": [
        {{
            "question_id": 1,
            "question": "Given [specific scenario], how would you [specific challenge]? Explain your approach step-by-step.",
            "skill_target": "Primary Excel skill being assessed", 
            "difficulty": "Easy/Medium/Hard",
            "scenario_context": "Detailed business situation",
            "requires_dataset": true/false,
            "dataset_requirements": {{
                "data_context": "marketing campaigns/sales data/employee records/etc",
                "required_columns": ["Column1", "Column2", "Column3"],
                "data_challenges": ["zero values", "missing data", "inconsistent formats"],
                "sample_size": 30-100
            }},
            "expected_approach": "Key Excel methodology elements",
            "evaluation_criteria": ["methodology clarity", "function appropriateness"],
            "excel_functions_tested": ["primary functions candidate should mention"]
        }}
    ],
    "total_questions": 8-10,
    "estimated_duration": "45-60 minutes", 
    "skills_covered": ["skill1", "skill2", ...],
    "assessment_focus": "Excel methodology and analytical thinking"
}}

SAMPLE QUESTION FOR ROAS CALCULATION:
"A marketing manager needs to calculate Return on Ad Spend (ROAS) for different campaigns. You have campaign data with 'Spend' and 'Revenue' columns, but some campaigns have zero spend values. How would you create a ROAS calculation (Revenue/Spend) that handles division by zero errors gracefully? Walk me through your Excel approach step-by-step, including the functions you'd use and your error-handling strategy."

Generate similar case-based questions that require realistic business data to make the Excel challenge meaningful."""

        try:
            start_time = time.time()
            response = self.generate_response(user_input, context)
            execution_time = time.time() - start_time
            
            if response["success"]:
                json_result = self.parse_json_response(response["response"])
                if json_result["success"]:
                    # Enhance the interview plan with contextual datasets
                    enhanced_plan = self._enhance_interview_plan_with_data(json_result["data"])
                    return {
                        "success": True,
                        "interview_plan": enhanced_plan,
                        "raw_response": response["response"]
                    }
                else:
                    logger.error(f"Failed to parse interview plan JSON: {json_result.get('error')}")
                    return {
                        "success": False,
                        "error": "Failed to parse interview plan JSON",
                        "raw_response": response["response"]
                    }
            else:
                return response
                
        except Exception as e:
            logger.error(f"Error generating case-based interview plan: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def _enhance_interview_plan_with_data(self, plan_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Enhance interview plan by generating contextual datasets for questions that need them
        """
        enhanced_plan = plan_data.copy()
        
        for question in enhanced_plan.get("interview_plan", []):
            if question.get("requires_dataset", False):
                try:
                    # Generate contextual dataset for this specific question
                    dataset_result = self._logged_generate_contextual_data(
                        question_context=question.get("question", ""),
                        size=question.get("dataset_requirements", {}).get("sample_size", 50),
                        specific_requirements=question.get("dataset_requirements", {})
                    )
                    
                    if dataset_result.get("success"):
                        question["dataset_info"] = {
                            "html_table": dataset_result["dataset_html"],
                            "csv_data": dataset_result["dataset_csv"],
                            "metadata": dataset_result["dataset_info"],
                            "context_analysis": dataset_result.get("context_analysis", {}),
                            "generated_successfully": True
                        }
                        logger.info(f"Generated contextual dataset for question {question.get('question_id')}")
                    else:
                        # Fallback to specific question type data
                        fallback_result = self.enhanced_data_generator.generate_for_specific_question_types(
                            question.get("skill_target", "general"),
                            size=30
                        )
                        
                        question["dataset_info"] = {
                            "html_table": fallback_result["dataset_html"],
                            "csv_data": fallback_result["dataset_csv"],
                            "metadata": fallback_result["dataset_info"],
                            "fallback_used": True,
                            "generated_successfully": True
                        }
                        logger.warning(f"Used fallback data for question {question.get('question_id')}")
                
                except Exception as e:
                    logger.error(f"Error generating dataset for question {question.get('question_id')}: {str(e)}")
                    question["dataset_info"] = {
                        "error": str(e),
                        "generated_successfully": False
                    }
        
        # Add assessment metadata
        enhanced_plan["assessment_metadata"] = {
            "focus_type": "case_based_methodology",
            "evaluation_approach": "verbal_explanation",
            "excel_access_required": False,
            "created_timestamp": time.time(),
            "agent_version": "Enhanced QueCraft v3.0",
            "data_generation_enabled": True
        }
        
        logger.info(f"Enhanced interview plan with {len(enhanced_plan['interview_plan'])} questions and contextual datasets")
        
        return enhanced_plan
    
    def create_single_question_with_data(self, 
                                       question_context: str,
                                       skill_target: str,
                                       difficulty: str = "Medium") -> Dict[str, Any]:
        """
        Create a single question with perfectly matched dataset
        
        Args:
            question_context (str): The specific question or scenario
            skill_target (str): Excel skill being tested
            difficulty (str): Question difficulty level
            
        Returns:
            dict: Complete question with contextual dataset
        """
        try:
            # Generate contextual dataset
            dataset_result = self._logged_generate_contextual_data(
                question_context=question_context,
                size=40,
                specific_requirements={
                    "skill_target": skill_target,
                    "difficulty": difficulty
                }
            )
            
            question_data = {
                "question_id": 1,
                "question": question_context,
                "skill_target": skill_target,
                "difficulty": difficulty,
                "requires_dataset": True,
                "dataset_info": dataset_result if dataset_result.get("success") else {"error": "Failed to generate data"}
            }
            
            return {
                "success": True,
                "question": question_data,
                "dataset_generated": dataset_result.get("success", False)
            }
            
        except Exception as e:
            logger.error(f"Error creating single question with data: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def refine_question_data_match(self, 
                                  question: Dict[str, Any],
                                  data_feedback: str) -> Dict[str, Any]:
        """
        Refine the data generation to better match the question requirements
        
        Args:
            question (dict): Original question data
            data_feedback (str): Feedback on how to improve data matching
            
        Returns:
            dict: Refined question with better matched dataset
        """
        try:
            # Regenerate data with specific feedback
            improved_dataset = self._logged_generate_contextual_data(
                question_context=question.get("question", ""),
                size=50,
                specific_requirements={
                    "improvement_feedback": data_feedback,
                    "original_question": question,
                    "skill_target": question.get("skill_target", "")
                }
            )
            
            # Update question with improved dataset
            refined_question = question.copy()
            refined_question["dataset_info"] = improved_dataset
            refined_question["data_refinement_applied"] = data_feedback
            
            return {
                "success": True,
                "refined_question": refined_question,
                "improvement_applied": True
            }
            
        except Exception as e:
            logger.error(f"Error refining question data match: {str(e)}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def validate_question_data_alignment(self, question: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate that the generated data properly supports the question
        
        Args:
            question (dict): Question with dataset
            
        Returns:
            dict: Validation results
        """
        try:
            dataset_info = question.get("dataset_info", {})
            if not dataset_info.get("generated_successfully"):
                return {
                    "aligned": False,
                    "issues": ["Dataset generation failed"],
                    "recommendation": "Retry data generation with specific requirements"
                }
            
            question_text = question.get("question", "").lower()
            dataset_columns = dataset_info.get("metadata", {}).get("column_names", [])
            
            # Check if question mentions specific columns/concepts that should be in data
            alignment_issues = []
            
            # Check for common Excel scenarios
            if "roas" in question_text and not any("spend" in col.lower() and "revenue" in col.lower() for col in dataset_columns):
                alignment_issues.append("ROAS question requires 'Spend' and 'Revenue' columns")
            
            if "vlookup" in question_text or "lookup" in question_text:
                if len(dataset_columns) < 4:
                    alignment_issues.append("Lookup questions typically need multiple tables/columns")
            
            if "pivot" in question_text:
                categorical_cols = [col for col in dataset_columns if "name" in col.lower() or "type" in col.lower()]
                if len(categorical_cols) < 2:
                    alignment_issues.append("Pivot questions need categorical columns for grouping")
            
            return {
                "aligned": len(alignment_issues) == 0,
                "issues": alignment_issues,
                "dataset_columns": dataset_columns,
                "validation_score": max(0, 100 - len(alignment_issues) * 25)
            }
            
        except Exception as e:
            logger.error(f"Error validating question-data alignment: {str(e)}")
            return {
                "aligned": False,
                "error": str(e)
            }

================
File: agents\recruiter_agent.py
================
"""
Recruiter Agent - Responsible for final hiring decisions and comprehensive assessment
"""

from .base_agent import BaseAgent
from utils.prompts import RECRUITER_SYSTEM_PROMPT
import json

class RecruiterAgent(BaseAgent):
    """
    Recruiter Agent makes final hiring decisions based on interview performance
    """
    
    def __init__(self, llm_client, tools=None):
        super().__init__("Recruiter", llm_client, tools)
    
    def get_system_prompt(self) -> str:
        """Get the system prompt for Recruiter agent"""
        return RECRUITER_SYSTEM_PROMPT
    
    def make_final_decision(self, interview_summary: dict, evaluation_history: list, role_requirements: dict = None) -> dict:
        """
        Make final hiring decision based on complete interview performance
        
        Args:
            interview_summary (dict): Summary of the interview session
            evaluation_history (list): List of all question evaluations
            role_requirements (dict, optional): Specific requirements for the role
            
        Returns:
            dict: Final decision and comprehensive assessment
        """
        context = {
            "interview_summary": interview_summary,
            "evaluation_history": evaluation_history,
            "role_requirements": role_requirements or {},
            "request_type": "final_decision"
        }
        
        # Calculate performance metrics
        performance_metrics = self._calculate_performance_metrics(evaluation_history)
        
        user_input = f"""Based on the complete interview performance, make a final hiring decision:

INTERVIEW SUMMARY:
{json.dumps(interview_summary, indent=2)}

PERFORMANCE METRICS:
{json.dumps(performance_metrics, indent=2)}

EVALUATION HISTORY:
{json.dumps(evaluation_history, indent=2)}

ROLE REQUIREMENTS:
{json.dumps(role_requirements, indent=2)}

Please provide a comprehensive final assessment in JSON format:
{{
    "decision": "Pass/Fail/Continue Interview",
    "confidence_level": "High/Medium/Low",
    "overall_score": 0-100,
    "recommendation_summary": "2-3 paragraph summary for hiring managers",
    "strengths": ["strength1", "strength2", ...],
    "weaknesses": ["weakness1", "weakness2", ...],
    "skill_assessment": {{
        "basic_formulas": "Excellent/Good/Fair/Poor",
        "data_analysis": "Excellent/Good/Fair/Poor",
        "pivot_tables": "Excellent/Good/Fair/Poor",
        "data_visualization": "Excellent/Good/Fair/Poor",
        "advanced_functions": "Excellent/Good/Fair/Poor"
    }},
    "improvement_areas": ["area1", "area2", ...],
    "follow_up_recommendations": ["recommendation1", "recommendation2", ...],
    "next_steps": "Recommended next steps in hiring process"
}}

Consider the candidate's:
1. Technical proficiency across Excel skills
2. Problem-solving approach and methodology
3. Communication clarity and explanation quality
4. Consistency in performance across different question types
5. Ability to handle increasing complexity"""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            json_result = self.parse_json_response(response["response"])
            if json_result["success"]:
                decision_data = json_result["data"]
                decision_data["performance_metrics"] = performance_metrics
                
                return {
                    "success": True,
                    "final_decision": decision_data,
                    "raw_response": response["response"]
                }
            else:
                return {
                    "success": False,
                    "error": "Failed to parse decision JSON",
                    "raw_response": response["response"]
                }
        else:
            return response
    
    def should_continue_interview(self, current_evaluations: list, question_count: int) -> dict:
        """
        Determine if interview should continue based on current performance
        
        Args:
            current_evaluations (list): Evaluations so far
            question_count (int): Number of questions asked
            
        Returns:
            dict: Decision on whether to continue
        """
        context = {
            "current_evaluations": current_evaluations,
            "question_count": question_count,
            "request_type": "continue_decision"
        }
        
        # Quick performance analysis
        performance_summary = self._analyze_current_performance(current_evaluations)
        
        user_input = f"""Based on the current interview progress, decide whether to continue:

QUESTIONS ASKED: {question_count}
CURRENT PERFORMANCE SUMMARY:
{json.dumps(performance_summary, indent=2)}

RECENT EVALUATIONS:
{json.dumps(current_evaluations[-3:], indent=2)}  # Last 3 evaluations

Please provide a decision in JSON format:
{{
    "continue_interview": true/false,
    "reason": "Explanation for the decision",
    "suggested_focus_area": "Area to focus on if continuing",
    "early_termination_reason": "Reason if stopping early",
    "confidence_in_decision": "High/Medium/Low"
}}

Continue if:
- Performance is improving or consistently good
- Still need to assess key skills
- Haven't reached minimum question threshold (5 questions)

Stop early if:
- Consistently poor performance (3+ unsatisfactory grades)
- Candidate clearly unprepared
- Major knowledge gaps evident"""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return self.parse_json_response(response["response"])
        else:
            return response
    
    def generate_feedback_report(self, final_decision: dict, interview_data: dict) -> dict:
        """
        Generate comprehensive feedback report for candidate
        
        Args:
            final_decision (dict): The final hiring decision
            interview_data (dict): Complete interview session data
            
        Returns:
            dict: Formatted feedback report
        """
        context = {
            "final_decision": final_decision,
            "interview_data": interview_data,
            "request_type": "feedback_report"
        }
        
        user_input = f"""Generate a constructive feedback report for the candidate:

FINAL DECISION:
{json.dumps(final_decision, indent=2)}

INTERVIEW DATA:
{json.dumps(interview_data, indent=2)}

Create a professional feedback report in JSON format:
{{
    "report_summary": "Overall performance summary",
    "detailed_feedback": {{
        "technical_skills": "Assessment of Excel technical skills",
        "problem_solving": "Problem-solving approach evaluation",
        "communication": "Communication and explanation quality",
        "areas_of_strength": ["strength1", "strength2", ...],
        "areas_for_improvement": ["improvement1", "improvement2", ...],
        "specific_recommendations": ["recommendation1", "recommendation2", ...]
    }},
    "skill_breakdown": {{
        "formulas_and_functions": {{"score": 0-100, "feedback": "specific feedback"}},
        "data_analysis": {{"score": 0-100, "feedback": "specific feedback"}},
        "pivot_tables": {{"score": 0-100, "feedback": "specific feedback"}},
        "data_visualization": {{"score": 0-100, "feedback": "specific feedback"}}
    }},
    "next_steps": "Recommended next steps for skill development",
    "interview_experience_rating": "Rate the interview experience for improvements"
}}"""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return self.parse_json_response(response["response"])
        else:
            return response
    
    def _calculate_performance_metrics(self, evaluation_history: list) -> dict:
        """
        Calculate performance metrics from evaluation history
        
        Args:
            evaluation_history (list): List of evaluations
            
        Returns:
            dict: Performance metrics
        """
        if not evaluation_history:
            return {"error": "No evaluations to analyze"}
        
        successful_evaluations = [e for e in evaluation_history if e.get("success", False)]
        
        if not successful_evaluations:
            return {"error": "No successful evaluations"}
        
        grades = []
        scores = []
        skills_tested = []
        
        for eval_item in successful_evaluations:
            evaluation = eval_item.get("evaluation", {})
            if evaluation:
                grades.append(evaluation.get("grade", ""))
                if "score" in evaluation:
                    scores.append(evaluation["score"])
                if "skill_target" in evaluation:
                    skills_tested.append(evaluation["skill_target"])
        
        # Calculate grade distribution
        grade_counts = {}
        for grade in grades:
            grade_counts[grade] = grade_counts.get(grade, 0) + 1
        
        # Calculate average score
        avg_score = sum(scores) / len(scores) if scores else 0
        
        # Determine trend (improving/declining/stable)
        trend = "stable"
        if len(scores) > 2:
            first_half_avg = sum(scores[:len(scores)//2]) / len(scores[:len(scores)//2])
            second_half_avg = sum(scores[len(scores)//2:]) / len(scores[len(scores)//2:])
            if second_half_avg > first_half_avg + 10:
                trend = "improving"
            elif second_half_avg < first_half_avg - 10:
                trend = "declining"
        
        return {
            "total_questions": len(successful_evaluations),
            "average_score": round(avg_score, 2),
            "grade_distribution": grade_counts,
            "skills_tested": list(set(skills_tested)),
            "performance_trend": trend,
            "pass_rate": (grade_counts.get("Satisfactory", 0) + grade_counts.get("Partly Acceptable", 0)) / len(grades) * 100 if grades else 0
        }
    
    def _analyze_current_performance(self, current_evaluations: list) -> dict:
        """
        Analyze current performance for continue/stop decision
        
        Args:
            current_evaluations (list): Current evaluations
            
        Returns:
            dict: Performance analysis
        """
        if not current_evaluations:
            return {"status": "no_data"}
        
        recent_grades = []
        for eval_item in current_evaluations[-3:]:  # Last 3 evaluations
            if eval_item.get("success") and eval_item.get("evaluation"):
                recent_grades.append(eval_item["evaluation"].get("grade", ""))
        
        unsatisfactory_count = recent_grades.count("Unsatisfactory")
        satisfactory_count = recent_grades.count("Satisfactory")
        
        return {
            "recent_grades": recent_grades,
            "unsatisfactory_streak": unsatisfactory_count,
            "satisfactory_count": satisfactory_count,
            "total_evaluated": len(current_evaluations),
            "recommendation": "stop" if unsatisfactory_count >= 2 else "continue"
        }

================
File: agents\reviewer_agent.py
================
"""
Reviewer Agent - Responsible for evaluating candidate responses
"""

from .base_agent import BaseAgent
from utils.prompts import REVIEWER_SYSTEM_PROMPT

class ReviewerAgent(BaseAgent):
    """
    Reviewer Agent evaluates candidate answers against Excel proficiency standards
    """
    
    def __init__(self, llm_client, tools=None):
        super().__init__("Reviewer", llm_client, tools)
    
    def get_system_prompt(self) -> str:
        """Get the system prompt for Reviewer agent"""
        return REVIEWER_SYSTEM_PROMPT
    
    def evaluate_response(self, question: dict, candidate_answer: str, context: dict = None) -> dict:
        """
        Evaluate a candidate's response to an Excel interview question
        
        Args:
            question (dict): The interview question details
            candidate_answer (str): The candidate's response
            context (dict, optional): Additional context for evaluation
            
        Returns:
            dict: Evaluation results with grade and justification
        """
        evaluation_context = {
            "question": question,
            "candidate_answer": candidate_answer,
            "additional_context": context or {},
            "request_type": "evaluate_response"
        }
        
        user_input = f"""Evaluate the following candidate response:

QUESTION:
- Question: {question.get('question', 'N/A')}
- Skill Target: {question.get('skill_target', 'N/A')}
- Difficulty: {question.get('difficulty', 'N/A')}
- Expected Approach: {question.get('expected_approach', 'N/A')}

CANDIDATE ANSWER:
{candidate_answer}

Please evaluate this response and return a JSON object with the following structure:
{{
    "grade": "Satisfactory/Partly Acceptable/Unsatisfactory/Requires More Assessment",
    "score": 0-100,
    "justification": "Detailed explanation of the grade",
    "strengths": ["strength1", "strength2", ...],
    "weaknesses": ["weakness1", "weakness2", ...],
    "alternative_solutions": ["Better approach 1", "Better approach 2", ...],
    "follow_up_needed": true/false,
    "follow_up_suggestion": "Suggested follow-up question if needed"
}}

Consider:
1. Correctness of the solution
2. Efficiency of the approach
3. Understanding of Excel concepts
4. Clarity of explanation
5. Practical applicability"""

        response = self.generate_response(user_input, evaluation_context)
        
        if response["success"]:
            json_result = self.parse_json_response(response["response"])
            if json_result["success"]:
                evaluation = json_result["data"]
                # Add metadata
                evaluation["question_id"] = question.get("question_id")
                evaluation["skill_target"] = question.get("skill_target")
                evaluation["timestamp"] = evaluation_context.get("timestamp")
                
                return {
                    "success": True,
                    "evaluation": evaluation,
                    "raw_response": response["response"]
                }
            else:
                return {
                    "success": False,
                    "error": "Failed to parse evaluation JSON",
                    "raw_response": response["response"]
                }
        else:
            return response
    
    def verify_excel_concept(self, concept: str, candidate_explanation: str) -> dict:
        """
        Verify candidate's understanding of a specific Excel concept
        
        Args:
            concept (str): The Excel concept to verify
            candidate_explanation (str): Candidate's explanation of the concept
            
        Returns:
            dict: Verification results
        """
        # Use web search tool if available to verify concept
        search_result = self.execute_tool("web_search", query=f"Excel {concept} syntax examples")
        
        context = {
            "concept": concept,
            "candidate_explanation": candidate_explanation,
            "search_result": search_result,
            "request_type": "verify_concept"
        }
        
        user_input = f"""Verify the candidate's understanding of the Excel concept: {concept}

Candidate's Explanation:
{candidate_explanation}

Please return a JSON object:
{{
    "concept_understood": true/false,
    "accuracy_score": 0-100,
    "explanation_quality": "Poor/Fair/Good/Excellent",
    "corrections_needed": ["correction1", "correction2", ...],
    "concept_verification": "Detailed verification notes"
}}"""

        response = self.generate_response(user_input, context)
        
        if response["success"]:
            return self.parse_json_response(response["response"])
        else:
            return response
    
    def batch_evaluate(self, question_answer_pairs: list) -> dict:
        """
        Evaluate multiple question-answer pairs in batch
        
        Args:
            question_answer_pairs (list): List of tuples (question, answer)
            
        Returns:
            dict: Batch evaluation results
        """
        evaluations = []
        
        for i, (question, answer) in enumerate(question_answer_pairs):
            evaluation = self.evaluate_response(question, answer)
            evaluation["pair_index"] = i
            evaluations.append(evaluation)
        
        # Calculate overall statistics
        successful_evaluations = [e for e in evaluations if e.get("success", False)]
        
        if successful_evaluations:
            grades = [e["evaluation"]["grade"] for e in successful_evaluations]
            scores = [e["evaluation"]["score"] for e in successful_evaluations if "score" in e["evaluation"]]
            
            grade_distribution = {}
            for grade in grades:
                grade_distribution[grade] = grade_distribution.get(grade, 0) + 1
            
            return {
                "success": True,
                "evaluations": evaluations,
                "summary": {
                    "total_evaluated": len(successful_evaluations),
                    "average_score": sum(scores) / len(scores) if scores else 0,
                    "grade_distribution": grade_distribution
                }
            }
        else:
            return {
                "success": False,
                "error": "No successful evaluations",
                "evaluations": evaluations
            }

================
File: agents\__init__.py
================
"""
AI-Powered Excel Mock Interviewer - Agents Module
"""

from agents.base_agent import BaseAgent
from agents.quecraft_agent import QueCraftAgent
from agents.reviewer_agent import ReviewerAgent
from agents.recruiter_agent import RecruiterAgent
from agents.interviewer_agent import InterviewerAgent

__all__ = [
    'BaseAgent',
    'QueCraftAgent', 
    'ReviewerAgent',
    'RecruiterAgent',
    'InterviewerAgent'
]

================
File: utils\json_parser.py
================
"""
JSON Parser utility for handling LLM responses and ensuring proper JSON formatting
"""

import json
import re
import logging
from typing import Dict, Any, Union

logger = logging.getLogger(__name__)

class JSONParser:
    """
    Utility class for parsing and validating JSON responses from LLM agents
    """
    
    @staticmethod
    def parse_json_response(response_text: str) -> Dict[str, Any]:
        """
        Parse JSON from LLM response text, handling common formatting issues
        
        Args:
            response_text (str): Raw response text from LLM
            
        Returns:
            dict: Parsed JSON data or error information
        """
        if not response_text or not isinstance(response_text, str):
            return {
                "success": False,
                "error": "Empty or invalid response text",
                "data": None
            }
        
        # Try direct JSON parsing first
        try:
            data = json.loads(response_text)
            return {
                "success": True,
                "data": data,
                "error": None
            }
        except json.JSONDecodeError:
            pass
        
        # Try to extract JSON from markdown code blocks
        json_match = re.search(r'```(?:json)?\s*(\{.*?\})\s*```', response_text, re.DOTALL)
        if json_match:
            try:
                data = json.loads(json_match.group(1))
                return {
                    "success": True,
                    "data": data,
                    "error": None
                }
            except json.JSONDecodeError:
                pass
        
        # Try to find JSON object in text
        json_pattern = r'\{(?:[^{}]|{(?:[^{}]|{[^{}]*})*})*\}'
        json_matches = re.findall(json_pattern, response_text, re.DOTALL)
        
        for match in json_matches:
            try:
                data = json.loads(match)
                return {
                    "success": True,
                    "data": data,
                    "error": None
                }
            except json.JSONDecodeError:
                continue
        
        # If all parsing attempts fail
        logger.error(f"Failed to parse JSON from response: {response_text[:200]}...")
        return {
            "success": False,
            "error": "Could not parse valid JSON from response",
            "data": None,
            "raw_response": response_text
        }
    
    @staticmethod
    def validate_interview_plan(data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate interview plan JSON structure
        
        Args:
            data (dict): Parsed JSON data
            
        Returns:
            dict: Validation results
        """
        required_fields = ["interview_plan", "total_questions", "estimated_duration", "skills_covered"]
        required_question_fields = ["question_id", "question", "skill_target", "difficulty"]
        
        validation_errors = []
        
        # Check required top-level fields
        for field in required_fields:
            if field not in data:
                validation_errors.append(f"Missing required field: {field}")
        
        # Validate interview_plan structure
        if "interview_plan" in data:
            if not isinstance(data["interview_plan"], list):
                validation_errors.append("interview_plan must be a list")
            else:
                for i, question in enumerate(data["interview_plan"]):
                    if not isinstance(question, dict):
                        validation_errors.append(f"Question {i+1} must be a dictionary")
                        continue
                    
                    for field in required_question_fields:
                        if field not in question:
                            validation_errors.append(f"Question {i+1} missing field: {field}")
                    
                    # Validate difficulty levels
                    if "difficulty" in question:
                        if question["difficulty"] not in ["Easy", "Medium", "Hard"]:
                            validation_errors.append(f"Question {i+1} has invalid difficulty level")
        
        return {
            "is_valid": len(validation_errors) == 0,
            "errors": validation_errors,
            "data": data
        }
    
    @staticmethod
    def validate_evaluation(data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate evaluation JSON structure
        
        Args:
            data (dict): Parsed JSON data
            
        Returns:
            dict: Validation results
        """
        required_fields = ["grade", "score", "justification", "strengths", "weaknesses"]
        valid_grades = ["Satisfactory", "Partly Acceptable", "Unsatisfactory", "Requires More Assessment"]
        
        validation_errors = []
        
        # Check required fields
        for field in required_fields:
            if field not in data:
                validation_errors.append(f"Missing required field: {field}")
        
        # Validate grade
        if "grade" in data:
            if data["grade"] not in valid_grades:
                validation_errors.append(f"Invalid grade: {data['grade']}")
        
        # Validate score
        if "score" in data:
            try:
                score = float(data["score"])
                if not 0 <= score <= 100:
                    validation_errors.append("Score must be between 0 and 100")
            except (TypeError, ValueError):
                validation_errors.append("Score must be a number")
        
        # Validate lists
        for list_field in ["strengths", "weaknesses"]:
            if list_field in data:
                if not isinstance(data[list_field], list):
                    validation_errors.append(f"{list_field} must be a list")
        
        return {
            "is_valid": len(validation_errors) == 0,
            "errors": validation_errors,
            "data": data
        }
    
    @staticmethod
    def validate_final_decision(data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Validate final decision JSON structure
        
        Args:
            data (dict): Parsed JSON data
            
        Returns:
            dict: Validation results
        """
        required_fields = ["decision", "confidence_level", "overall_score", "recommendation_summary"]
        valid_decisions = ["Pass", "Fail", "Continue Interview"]
        valid_confidence = ["High", "Medium", "Low"]
        
        validation_errors = []
        
        # Check required fields
        for field in required_fields:
            if field not in data:
                validation_errors.append(f"Missing required field: {field}")
        
        # Validate decision
        if "decision" in data:
            if data["decision"] not in valid_decisions:
                validation_errors.append(f"Invalid decision: {data['decision']}")
        
        # Validate confidence level
        if "confidence_level" in data:
            if data["confidence_level"] not in valid_confidence:
                validation_errors.append(f"Invalid confidence level: {data['confidence_level']}")
        
        # Validate overall score
        if "overall_score" in data:
            try:
                score = float(data["overall_score"])
                if not 0 <= score <= 100:
                    validation_errors.append("Overall score must be between 0 and 100")
            except (TypeError, ValueError):
                validation_errors.append("Overall score must be a number")
        
        return {
            "is_valid": len(validation_errors) == 0,
            "errors": validation_errors,
            "data": data
        }
    
    @staticmethod
    def clean_response_text(text: str) -> str:
        """
        Clean response text for better JSON parsing
        
        Args:
            text (str): Raw response text
            
        Returns:
            str: Cleaned text
        """
        if not text:
            return ""
        
        # Remove common prefixes and suffixes
        text = re.sub(r'^Here\'s.*?:\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'^```json\s*', '', text, flags=re.IGNORECASE)
        text = re.sub(r'\s*```$', '', text)
        
        # Remove extra whitespace
        text = text.strip()
        
        return text
    
    @staticmethod
    def format_json_for_display(data: Union[Dict, str], indent: int = 2) -> str:
        """
        Format JSON data for display
        
        Args:
            data: JSON data or string
            indent: Indentation level
            
        Returns:
            str: Formatted JSON string
        """
        try:
            if isinstance(data, str):
                data = json.loads(data)
            return json.dumps(data, indent=indent, ensure_ascii=False)
        except (json.JSONDecodeError, TypeError):
            return str(data)

================
File: utils\mock_data_generator.py
================
"""
Enhanced Mock Data Generator with LLM integration for contextually relevant data
"""

import pandas as pd
import random
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union
from faker import Faker
import io
import json

logger = logging.getLogger(__name__)

class MockDataGenerator:
    """
    Enhanced data generator that uses LLM to create contextually relevant datasets
    specifically designed for Excel interview questions
    """
    
    def __init__(self, llm_client=None):
        self.fake = Faker()
        self.llm_client = llm_client
        Faker.seed(42)
        
        logger.info("Enhanced Mock Data Generator initialized with LLM integration")
    
    def generate_contextual_dataset(self, 
                                  question_context: str,
                                  size: int = 50,
                                  specific_requirements: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        Generate dataset that matches the specific question context using LLM
        
        Args:
            question_context (str): The interview question requiring data
            size (int): Number of records
            specific_requirements (dict): Specific data requirements
            
        Returns:
            dict: Generated dataset with HTML table format
        """
        logger.info(f"Generating contextual dataset for question: {question_context[:100]}...")
        
        try:
            # Use LLM to understand context and generate appropriate data structure
            data_structure = self._analyze_question_and_design_data(question_context, specific_requirements)
            
            if not data_structure.get("success"):
                logger.warning("Failed to analyze question context, falling back to sales data")
                return self._generate_fallback_data(size)
            
            # Generate the actual data based on LLM analysis
            dataset = self._generate_data_from_structure(data_structure["structure"], size)
            
            # Convert to HTML table format
            html_table = self._convert_to_html_table(dataset)
            
            return {
                "success": True,
                "dataset_html": html_table,
                "dataset_csv": dataset.to_csv(index=False),
                "dataset_info": self._get_dataset_metadata(dataset),
                "context_analysis": data_structure.get("analysis", {}),
                "columns": dataset.columns.tolist(),
                "rows": len(dataset)
            }
            
        except Exception as e:
            logger.error(f"Error generating contextual dataset: {str(e)}")
            fallback_data = self._generate_fallback_data(size)
            return {
                "success": False,
                "error": str(e),
                "dataset_html": self._convert_to_html_table(fallback_data["dataset"]),
                "dataset_csv": fallback_data["dataset"].to_csv(index=False),
                "fallback_used": True
            }
    
    def _analyze_question_and_design_data(self, question_context: str, requirements: Dict = None) -> Dict[str, Any]:
        """
        Use LLM to analyze question and design appropriate data structure
        """
        if not self.llm_client:
            return {"success": False, "error": "No LLM client available"}
        
        prompt = f"""
Analyze this Excel interview question and design a dataset structure that would be perfect for testing the skills mentioned:

QUESTION: {question_context}

ADDITIONAL REQUIREMENTS: {json.dumps(requirements or {}, indent=2)}

Design a dataset structure that:
1. Directly supports the Excel challenge described in the question
2. Contains the exact columns mentioned or implied in the question
3. Includes realistic business data relevant to the scenario
4. Has some data quality issues that make the Excel challenge meaningful
5. Is sized appropriately for the skill being tested
6. Has only 10-20 rows

Respond in JSON format:
{{
    "analysis": {{
        "question_type": "data_analysis/formula_creation/pivot_tables/data_cleaning/etc",
        "key_skills_tested": ["skill1", "skill2", "skill3"],
        "data_context": "marketing/sales/hr/finance/etc",
        "specific_challenges": ["challenge1", "challenge2"]
    }},
    "structure": {{
        "columns": [
            {{
                "name": "Column_Name",
                "type": "string/number/date/boolean",
                "description": "what this column represents",
                "data_pattern": "how to generate realistic data",
                "include_issues": true/false,
                "issue_type": "missing_values/inconsistent_format/duplicates/etc"
            }}
        ],
        "business_context": "realistic business scenario description",
        "sample_size_recommendation": 10-20,
        "key_data_relationships": "how columns relate to each other"
    }}
}}

Make sure the dataset directly enables the candidate to practice the Excel skills mentioned in the question.
"""
        
        try:
            response = self.llm_client.generate_content(contents=prompt)
            
            # Parse the JSON response
            import re
            json_match = re.search(r'\{.*\}', response.text, re.DOTALL)
            if json_match:
                data_structure = json.loads(json_match.group(0))
                return {
                    "success": True,
                    "structure": data_structure,
                    "analysis": data_structure.get("analysis", {}),
                    "raw_response": response.text
                }
            else:
                logger.error("Could not extract JSON from LLM response")
                return {"success": False, "error": "Invalid LLM response format"}
                
        except Exception as e:
            logger.error(f"Error in LLM analysis: {str(e)}")
            return {"success": False, "error": str(e)}
    
    def _generate_data_from_structure(self, structure: Dict, size: int) -> pd.DataFrame:
        """
        Generate pandas DataFrame based on LLM-designed structure
        """
        data = []
        columns_info = structure["structure"]["columns"]
        
        for i in range(size):
            row = {}
            
            for col_info in columns_info:
                col_name = col_info["name"]
                col_type = col_info["type"]
                data_pattern = col_info.get("data_pattern", "")
                include_issues = col_info.get("include_issues", False)
                issue_type = col_info.get("issue_type", "")
                
                # Generate data based on column type and pattern
                value = self._generate_column_value(col_type, data_pattern, include_issues, issue_type, i)
                row[col_name] = value
            
            data.append(row)
        
        return pd.DataFrame(data)
    
    def _generate_column_value(self, col_type: str, pattern: str, include_issues: bool, issue_type: str, row_index: int):
        """
        Generate individual column values based on specifications
        """
        # Base value generation
        if col_type == "string" and col_type != None:
            if "campaign" in pattern.lower():
                base_value = f"{random.choice(['Email', 'Social', 'PPC', 'Display'])} Campaign {row_index + 1}"
            elif "name" in pattern.lower():
                base_value = self.fake.name()
            elif "company" in pattern.lower():
                base_value = self.fake.company()
            else:
                base_value = self.fake.word().title()
                
        elif col_type == "number" and col_type != None:
            if "spend" in pattern.lower():
                base_value = round(random.uniform(100, 10000), 2)
            elif "revenue" in pattern.lower():
                base_value = round(random.uniform(500, 50000), 2)
            elif "quantity" in pattern.lower():
                base_value = random.randint(1, 100)
            else:
                base_value = round(random.uniform(1, 1000), 2)
                
        elif col_type == "date" and col_type != None:
            base_value = self.fake.date_between(start_date="-1y", end_date="today").strftime("%Y-%m-%d")
            
        elif col_type == "boolean" and col_type != None:
            base_value = random.choice([True, False])
            
        else:
            base_value = str(self.fake.word())
        
        # Apply data quality issues
        if include_issues and random.random() < 0.15:  # 15% chance of issues
            if issue_type == "missing_values":
                return ""
            elif issue_type == "inconsistent_format" and col_type == "string":
                if random.random() < 0.5:
                    return str(base_value).upper()
                else:
                    return str(base_value).lower()
            elif issue_type == "zero_values" and col_type == "number":
                return 0
        
        return base_value
    
    def _convert_to_html_table(self, df: pd.DataFrame) -> str:
        """
        Convert DataFrame to HTML table with Bootstrap styling
        """
        html = df.to_html(
            index=False,
            classes='table table-striped table-bordered',
            escape=False
        )
        
        # Wrap in a container for better styling
        wrapped_html = f"""
        <div class="table-responsive">
            {html}
        </div>
        """
        
        return wrapped_html
    
    def _get_dataset_metadata(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        Generate metadata about the dataset
        """
        return {
            "rows": len(df),
            "columns": len(df.columns),
            "column_names": df.columns.tolist(),
            "data_types": {col: str(dtype) for col, dtype in df.dtypes.items()},
            "sample_preview": df.head(3).to_dict('records'),
            "has_missing_values": df.isnull().sum().sum() > 0,
            "memory_usage": f"{df.memory_usage().sum()} bytes"
        }
    
    def _generate_fallback_data(self, size: int) -> Dict[str, Any]:
        """
        Generate fallback data when LLM analysis fails
        """
        # Simple marketing campaign data as fallback
        data = []
        for i in range(size):
            spend = round(random.uniform(100, 10000), 2)
            # Some campaigns with zero spend for division by zero practice
            if random.random() < 0.1:  # 10% zero spend
                spend = 0
            
            revenue = round(random.uniform(500, 50000), 2) if spend > 0 else 0
            
            data.append({
                "Campaign_ID": f"CAM{str(i+1001).zfill(4)}",
                "Campaign_Name": f"{random.choice(['Email', 'Social', 'PPC', 'Display'])} Campaign {i+1}",
                "Spend": spend,
                "Revenue": revenue,
                "Impressions": random.randint(1000, 100000),
                "Clicks": random.randint(50, 5000),
                "Date": self.fake.date_between(start_date="-6m", end_date="today").strftime("%Y-%m-%d")
            })
        
        df = pd.DataFrame(data)
        return {
            "success": True,
            "dataset": df,
            "fallback_used": True
        }
    
    def generate_for_specific_question_types(self, question_type: str, size: int = 50) -> Dict[str, Any]:
        """
        Generate data for specific question types without LLM
        """
        if question_type.lower() in ["roas", "marketing", "campaign"]:
            return self._generate_marketing_roas_data(size)
        elif question_type.lower() in ["vlookup", "lookup", "index_match"]:
            return self._generate_lookup_data(size)
        elif question_type.lower() in ["pivot", "summary", "analysis"]:
            return self._generate_pivot_data(size)
        else:
            return self._generate_fallback_data(size)
    
    def _generate_marketing_roas_data(self, size: int) -> Dict[str, Any]:
        """
        Generate marketing ROAS data specifically for ROAS calculation questions
        """
        data = []
        campaigns = ["Email Marketing", "Social Media", "Google Ads", "Facebook Ads", "Display Banner", "YouTube"]
        
        for i in range(size):
            spend = round(random.uniform(0, 10000), 2)
            
            # Ensure some zero spend values for division by zero practice
            if random.random() < 0.15:  # 15% chance of zero spend
                spend = 0
            
            # Generate revenue based on spend (with some randomness)
            if spend > 0:
                roas_multiplier = random.uniform(0.5, 8.0)  # ROAS between 0.5 and 8
                revenue = round(spend * roas_multiplier, 2)
            else:
                revenue = round(random.uniform(0, 1000), 2)  # Some revenue even with zero spend
            
            data.append({
                "Campaign_ID": f"CAM{str(i+1001).zfill(4)}",
                "Campaign_Name": f"{random.choice(campaigns)} - Q{random.randint(1,4)} 2024",
                "Spend": spend,
                "Revenue": revenue,
                "Campaign_Type": random.choice(campaigns),
                "Start_Date": self.fake.date_between(start_date="-1y", end_date="-30d").strftime("%Y-%m-%d"),
                "End_Date": self.fake.date_between(start_date="-30d", end_date="today").strftime("%Y-%m-%d"),
                "Target_Audience": random.choice(["18-25", "26-35", "36-45", "46-55", "55+"]),
                "Platform": random.choice(["Google", "Facebook", "LinkedIn", "Twitter", "YouTube"])
            })
        
        df = pd.DataFrame(data)
        html_table = self._convert_to_html_table(df)
        
        return {
            "success": True,
            "dataset_html": html_table,
            "dataset_csv": df.to_csv(index=False),
            "dataset_info": self._get_dataset_metadata(df),
            "context_analysis": {
                "question_type": "ROAS Calculation",
                "key_challenges": ["Division by zero handling", "Formula creation", "Error checking"],
                "excel_functions_needed": ["IF", "ISERROR", "DIVIDE", "ROUND"]
            }
        }
    
    def _generate_lookup_data(self, size: int) -> Dict[str, Any]:
        """
        Generate data suitable for VLOOKUP/INDEX-MATCH exercises
        """
        # Generate main data table
        main_data = []
        product_codes = [f"PRD{str(i+1001).zfill(3)}" for i in range(20)]
        
        for i in range(size):
            main_data.append({
                "Order_ID": f"ORD{str(i+2001).zfill(4)}",
                "Product_Code": random.choice(product_codes),
                "Quantity": random.randint(1, 50),
                "Order_Date": self.fake.date_between(start_date="-6m", end_date="today").strftime("%Y-%m-%d"),
                "Customer_ID": f"CUST{random.randint(1001, 1100)}"
            })
        
        # Generate lookup table
        lookup_data = []
        for code in product_codes:
            lookup_data.append({
                "Product_Code": code,
                "Product_Name": self.fake.catch_phrase(),
                "Unit_Price": round(random.uniform(10, 500), 2),
                "Category": random.choice(["Electronics", "Clothing", "Home", "Sports", "Books"])
            })
        
        main_df = pd.DataFrame(main_data)
        lookup_df = pd.DataFrame(lookup_data)
        
        # Combine both tables with a separator
        combined_html = f"""
        <h4>Main Orders Data</h4>
        {self._convert_to_html_table(main_df)}
        
        <h4>Product Lookup Table</h4>
        {self._convert_to_html_table(lookup_df)}
        """
        
        return {
            "success": True,
            "dataset_html": combined_html,
            "dataset_csv": main_df.to_csv(index=False) + "\n\nLookup Table:\n" + lookup_df.to_csv(index=False),
            "dataset_info": {
                "main_table": self._get_dataset_metadata(main_df),
                "lookup_table": self._get_dataset_metadata(lookup_df)
            }
        }

================
File: utils\prompts.py
================
"""
Enhanced System prompts for case-based Excel skills assessment
Focus on interrogative, scenario-based questions without assuming Excel access
"""

QUERAFT_SYSTEM_PROMPT = """You are QueCraft, an expert Excel curriculum designer specializing in case-based, interrogative assessment. Your role is to create scenario-driven interview questions that test Excel thinking and problem-solving without assuming the candidate has direct access to Excel software.

**Core Responsibility:**
Design 8-10 progressive, case-based Excel questions that evaluate analytical thinking, Excel methodology knowledge, and practical problem-solving approaches through verbal explanations.

**Question Design Philosophy:**
- **Scenario-Based**: Present realistic business situations requiring Excel solutions
- **Interrogative Format**: Ask "How would you..." rather than "Please do..."
- **Problem-Solving Focus**: Evaluate thought process and methodology over execution
- **Progressive Difficulty**: Start with basic scenarios, advance to complex multi-step problems
- **Excel Thinking**: Test understanding of Excel capabilities, limitations, and best practices

**Question Format Template:**
"Given [REALISTIC BUSINESS SCENARIO], how would you [SPECIFIC EXCEL CHALLENGE]? Explain your approach step-by-step, including what Excel functions or features you would use and why."

**Scenario Categories:**
1. **Data Cleaning & Preparation**: Inconsistent formats, missing data, duplicates
2. **Formula & Function Logic**: Complex calculations, conditional logic, error handling
3. **Data Analysis**: Summarization, comparison, trend identification
4. **Lookup & Reference**: Finding, matching, and retrieving data across datasets
5. **Pivot Analysis**: Summarizing large datasets, dynamic reporting
6. **Data Visualization**: Chart selection, dashboard creation, visual storytelling
7. **Data Validation**: Ensuring data integrity, preventing errors
8. **Advanced Techniques**: Array formulas, dynamic ranges, automation concepts

**Sample Question Structures:**

**Basic Level:**
"A sales manager has received a customer list where names are formatted inconsistently - some are 'First Last', others are 'Last, First', and some are in all caps. How would you standardize these names into a consistent 'First Last' format? Walk me through your Excel approach."

**Intermediate Level:**
"You have a dataset of 500 sales transactions with columns for Date, Salesperson, Product, Quantity, and Unit Price. Your manager needs a summary showing total revenue by salesperson for each product category, but only for transactions from the last quarter. How would you approach this analysis? What Excel tools would you use?"

**Advanced Level:**
"A marketing team has campaign data with Spend, Impressions, and Conversions columns. They need to calculate Cost Per Mille (CPM) and Conversion Rate, but some campaigns have zero impressions or conversions. How would you create these calculated fields while handling division-by-zero errors gracefully? Describe your formula approach and error-handling strategy."

**Assessment Criteria for Each Question:**
- Excel function knowledge appropriateness
- Problem decomposition and logical thinking
- Step-by-step methodology clarity
- Awareness of Excel limitations and alternatives
- Error handling and edge case consideration
- Efficiency and best practice awareness

**Data Integration Instructions:**
When questions require datasets, use the generate_mock_data tool with specific parameters:
- Specify data type based on scenario context
- Include realistic data quality issues for cleaning exercises
- Ensure data supports the specific Excel challenge being tested
- Provide clear description of the dataset context in the question

**Question Output Format:**
{{
    "question_id": 1,
    "question": "Given [scenario], how would you [challenge]? Explain your approach...",
    "skill_target": "Primary Excel skill being assessed",
    "difficulty": "Easy/Medium/Hard",
    "scenario_context": "Business context description",
    "requires_dataset": true/false,
    "dataset_description": "Specific data needed for this scenario",
    "expected_approach": "Key Excel functions/methods candidate should mention",
    "evaluation_criteria": ["criterion1", "criterion2", ...],
    "follow_up_questions": ["What if...", "How would you handle..."]
}}

**Tools Available:**
- generate_mock_data: Create realistic datasets with intentional data quality issues
- web_search: Research current Excel best practices and real-world scenarios

Remember: Questions should test Excel THINKING and METHODOLOGY, not software operation. Focus on the candidate's analytical approach and Excel knowledge."""

REVIEWER_SYSTEM_PROMPT = """You are a Technical Reviewer specializing in evaluating Excel methodology and problem-solving approaches through verbal explanations. You assess candidates' Excel thinking, not their ability to execute functions directly.

**Core Responsibility:**
Evaluate candidate responses for Excel methodology understanding, problem-solving logic, and practical application knowledge based on their verbal explanations of approach.

**Evaluation Framework:**

**Excel Methodology (40%)**
- Appropriate function selection for the scenario
- Understanding of Excel capabilities and limitations  
- Knowledge of multiple solution approaches
- Awareness of performance implications

**Problem-Solving Logic (30%)**
- Step-by-step breakdown of complex problems
- Logical sequence of Excel operations
- Consideration of data preparation needs
- Handling of edge cases and error scenarios

**Practical Application (20%)**
- Real-world applicability of proposed solution
- Scalability considerations
- Data integrity and validation awareness
- Best practices implementation

**Communication Quality (10%)**
- Clarity in explaining Excel approach
- Proper use of Excel terminology
- Ability to justify function choices
- Completeness of solution description

**Evaluation Process:**
1. Analyze the candidate's proposed Excel methodology
2. Assess logical flow and completeness of approach
3. Evaluate appropriateness of suggested functions/features
4. Consider alternative solutions and optimizations
5. Check for awareness of potential issues or limitations

**Grading Scale:**
- **Satisfactory (80-100%)**: Demonstrates strong Excel methodology, logical approach, considers edge cases
- **Partly Acceptable (60-79%)**: Good understanding but misses some steps or has minor inefficiencies
- **Unsatisfactory (40-59%)**: Significant gaps in Excel knowledge or flawed logic
- **Requires More Assessment (<40%)**: Major misunderstanding requiring follow-up questions

**Response Format:**
{{
    "grade": "Satisfactory/Partly Acceptable/Unsatisfactory/Requires More Assessment",
    "score": 0-100,
    "justification": "Detailed explanation focusing on Excel methodology",
    "strengths": ["Excel knowledge demonstrated", "logical thinking", ...],
    "weaknesses": ["missing considerations", "inefficient approach", ...],
    "alternative_solutions": ["more efficient methods", "better practices", ...],
    "excel_functions_mentioned": ["functions candidate referenced"],
    "missing_considerations": ["important aspects not addressed"],
    "follow_up_needed": true/false,
    "follow_up_suggestion": "specific clarifying question"
}}

**Key Assessment Points:**
- Does the candidate understand WHEN to use specific Excel functions?
- Can they break down complex problems into manageable Excel steps?
- Do they consider data quality and preparation needs?
- Are they aware of Excel limitations and workarounds?
- Do they think about scalability and maintenance?

Focus on the THINKING behind the Excel approach, not perfect syntax recall."""

RECRUITER_SYSTEM_PROMPT = """You are the Final Recruiter making comprehensive hiring decisions based on Excel methodology assessment and problem-solving capability demonstrated through verbal explanations and case-based scenarios.

**Decision-Making Framework:**

**Excel Proficiency Assessment (50%)**
- Breadth of Excel function knowledge
- Understanding of appropriate tool selection
- Awareness of Excel capabilities and limitations
- Knowledge of best practices and efficiency considerations

**Analytical Problem-Solving (30%)**
- Ability to break down complex business scenarios
- Logical step-by-step methodology
- Consideration of edge cases and error handling
- Creative problem-solving within Excel constraints

**Business Application (20%)**
- Understanding of real-world Excel usage
- Ability to translate business needs into Excel solutions
- Awareness of data integrity and validation needs
- Scalability and maintenance considerations

**Pass Criteria:**
- Demonstrates solid Excel methodology across 70%+ of assessed areas
- Shows logical problem-solving approach consistently
- Understands appropriate Excel function selection
- Considers practical business application needs
- Communicates Excel approaches clearly

**Fail Criteria:**
- Significant gaps in fundamental Excel knowledge
- Poor problem-solving logic or incomplete methodology
- Inability to match Excel tools to business scenarios
- Consistent misunderstanding of Excel capabilities
- Cannot explain Excel approaches clearly

**Assessment Output:**
{{
    "decision": "Pass/Fail/Continue Interview",
    "confidence_level": "High/Medium/Low", 
    "overall_score": 0-100,
    "recommendation_summary": "Executive summary for hiring managers",
    "excel_competency_assessment": {{
        "basic_functions": "Excellent/Good/Fair/Poor",
        "data_manipulation": "Excellent/Good/Fair/Poor", 
        "lookup_functions": "Excellent/Good/Fair/Poor",
        "data_analysis": "Excellent/Good/Fair/Poor",
        "problem_solving": "Excellent/Good/Fair/Poor"
    }},
    "strengths": ["key strengths in Excel thinking"],
    "improvement_areas": ["areas needing development"],
    "practical_readiness": "assessment of job readiness"
}}

**Decision Factors:**
- Consistency across different Excel scenario types
- Depth of understanding beyond basic function knowledge
- Ability to handle increasing complexity in problems
- Business judgment in Excel solution selection
- Growth potential and learning indicators

Focus on Excel THINKING CAPABILITY and METHODOLOGY UNDERSTANDING rather than software execution skills."""

INTERVIEWER_SYSTEM_PROMPT = """You are a Professional Interviewer conducting case-based Excel skills assessment through conversational scenarios. Your role is to present business situations and guide candidates through explaining their Excel problem-solving approach.

**Interview Approach:**
- Present realistic business scenarios requiring Excel solutions
- Ask candidates to explain their methodology, not demonstrate software
- Use follow-up questions to probe deeper into their Excel thinking  
- Maintain supportive atmosphere while gathering assessment data
- Focus on understanding their analytical and Excel knowledge

**Question Presentation Style:**
"I'd like to present you with a business scenario and hear how you would approach solving it using Excel. [Scenario description]. How would you tackle this challenge? Please walk me through your thinking and the Excel tools you'd use."

**Scenario Introduction Examples:**
- "Let me describe a data situation you might encounter..."
- "Here's a business challenge that requires Excel analysis..."
- "I'm going to present a scenario involving data management..."
- "Consider this real-world Excel problem..."

**Follow-up Probing:**
- "What Excel functions would you use for that step?"
- "How would you handle [specific complication]?"
- "What if the dataset was much larger?"
- "Are there alternative approaches you'd consider?"
- "How would you validate your results?"

**Response Acknowledgment:**
- "That's an interesting approach. Can you elaborate on..."
- "I see your thinking. What about when..."
- "Good methodology. How would you handle..."
- "That makes sense. Walk me through the next step..."

**Encouragement Techniques:**
- "Take your time to think through the approach..."
- "Don't worry about perfect syntax, focus on the methodology..."
- "Think about what Excel tools would be most appropriate..."
- "Consider the business context as you plan your approach..."

**Interview Flow Management:**
- Present scenario clearly with necessary business context
- Allow thinking time before expecting detailed responses
- Use probing questions to assess depth of knowledge
- Smoothly transition between different Excel skill areas
- Maintain conversational flow while covering assessment areas

**Key Principles:**
- Test Excel THINKING, not software execution
- Use realistic business contexts for all scenarios
- Encourage explanation of methodology and reasoning
- Probe for understanding of Excel capabilities and limitations
- Assess problem-solving approach through verbal explanations

Your goal is to create an environment where candidates can demonstrate their Excel knowledge and analytical thinking through clear verbal explanations of their problem-solving methodology."""

================
File: utils\report_generator.py
================
"""
Report Generator for creating comprehensive interview reports and feedback
"""

from datetime import datetime
from typing import Dict, List, Any, Optional
import json

class ReportGenerator:
    """
    Generates various types of reports for the Excel interview system
    """
    
    def __init__(self):
        self.report_templates = {
            "candidate_feedback": self._candidate_feedback_template,
            "hiring_manager": self._hiring_manager_template,
            "interview_summary": self._interview_summary_template,
            "performance_analysis": self._performance_analysis_template
        }
    
    def generate_candidate_feedback_report(self, 
                                          candidate_name: str,
                                          final_decision: Dict[str, Any],
                                          interview_data: Dict[str, Any],
                                          evaluations: List[Dict[str, Any]]) -> str:
        """
        Generate comprehensive feedback report for candidate
        
        Args:
            candidate_name (str): Name of the candidate
            final_decision (dict): Final hiring decision data
            interview_data (dict): Complete interview session data
            evaluations (list): List of question evaluations
            
        Returns:
            str: Formatted feedback report
        """
        
        report = f"""
# Excel Proficiency Assessment - Candidate Feedback Report

**Candidate:** {candidate_name}
**Assessment Date:** {datetime.now().strftime('%B %d, %Y')}
**Interview Duration:** {interview_data.get('duration', 'N/A')}
**Questions Completed:** {len(evaluations)}

## Overall Performance Summary

**Final Result:** {final_decision.get('decision', 'N/A')}
**Overall Score:** {final_decision.get('overall_score', 'N/A')}/100
**Assessment Confidence:** {final_decision.get('confidence_level', 'N/A')}

{final_decision.get('recommendation_summary', '')}

## Skill Area Assessment

"""
        
        # Add skill breakdown
        skill_assessment = final_decision.get('skill_assessment', {})
        for skill, rating in skill_assessment.items():
            skill_display = skill.replace('_', ' ').title()
            report += f"**{skill_display}:** {rating}\n"
        
        report += "\n## Detailed Performance Analysis\n\n"
        
        # Add question-by-question feedback
        for i, evaluation in enumerate(evaluations, 1):
            if evaluation.get('success') and evaluation.get('evaluation'):
                eval_data = evaluation['evaluation']
                report += f"### Question {i}: {eval_data.get('skill_target', 'Unknown Skill')}\n\n"
                report += f"**Grade:** {eval_data.get('grade', 'N/A')}\n"
                report += f"**Score:** {eval_data.get('score', 'N/A')}/100\n\n"
                report += f"**Feedback:** {eval_data.get('justification', 'No feedback available')}\n\n"
                
                if eval_data.get('strengths'):
                    report += "**Strengths Demonstrated:**\n"
                    for strength in eval_data['strengths']:
                        report += f"• {strength}\n"
                    report += "\n"
                
                if eval_data.get('weaknesses'):
                    report += "**Areas for Improvement:**\n"
                    for weakness in eval_data['weaknesses']:
                        report += f"• {weakness}\n"
                    report += "\n"
        
        # Add recommendations
        report += "## Development Recommendations\n\n"
        if final_decision.get('improvement_areas'):
            for area in final_decision['improvement_areas']:
                report += f"• {area}\n"
        
        if final_decision.get('follow_up_recommendations'):
            report += "\n## Next Steps\n\n"
            for rec in final_decision['follow_up_recommendations']:
                report += f"• {rec}\n"
        
        report += f"\n\n---\n*Report generated on {datetime.now().strftime('%Y-%m-%d at %H:%M:%S')}*"
        
        return report
    
    def generate_hiring_manager_report(self,
                                      candidate_name: str,
                                      role_title: str,
                                      final_decision: Dict[str, Any],
                                      performance_metrics: Dict[str, Any],
                                      interview_data: Dict[str, Any]) -> str:
        """
        Generate executive summary report for hiring managers
        
        Args:
            candidate_name (str): Name of the candidate
            role_title (str): Position being interviewed for
            final_decision (dict): Final hiring decision
            performance_metrics (dict): Performance analysis
            interview_data (dict): Interview session data
            
        Returns:
            str: Executive summary report
        """
        
        report = f"""
# Hiring Manager Report - Excel Proficiency Assessment

## Candidate Summary
- **Candidate:** {candidate_name}
- **Position:** {role_title}
- **Assessment Date:** {datetime.now().strftime('%B %d, %Y')}
- **Interview Duration:** {interview_data.get('duration', 'N/A')}

## Executive Summary

**RECOMMENDATION: {final_decision.get('decision', 'N/A').upper()}**
**Confidence Level:** {final_decision.get('confidence_level', 'N/A')}
**Overall Score:** {final_decision.get('overall_score', 'N/A')}/100

{final_decision.get('recommendation_summary', '')}

## Performance Metrics

- **Questions Answered:** {performance_metrics.get('total_questions', 0)}
- **Average Score:** {performance_metrics.get('average_score', 0)}/100
- **Pass Rate:** {performance_metrics.get('pass_rate', 0):.1f}%
- **Performance Trend:** {performance_metrics.get('performance_trend', 'N/A').title()}

### Grade Distribution
"""
        
        # Add grade distribution
        grade_dist = performance_metrics.get('grade_distribution', {})
        for grade, count in grade_dist.items():
            report += f"- **{grade}:** {count} question(s)\n"
        
        report += "\n## Key Strengths\n\n"
        if final_decision.get('strengths'):
            for strength in final_decision['strengths']:
                report += f"• {strength}\n"
        
        report += "\n## Areas of Concern\n\n"
        if final_decision.get('weaknesses'):
            for weakness in final_decision['weaknesses']:
                report += f"• {weakness}\n"
        
        # Add next steps
        report += f"\n## Recommended Next Steps\n\n{final_decision.get('next_steps', 'No specific next steps provided.')}"
        
        report += f"\n\n---\n*Executive Report generated on {datetime.now().strftime('%Y-%m-%d at %H:%M:%S')}*"
        
        return report
    
    def generate_interview_summary(self, interview_data: Dict[str, Any]) -> str:
        """
        Generate technical interview summary
        
        Args:
            interview_data (dict): Complete interview session data
            
        Returns:
            str: Technical summary report
        """
        
        report = f"""
# Interview Session Summary

**Session ID:** {interview_data.get('session_id', 'N/A')}
**Start Time:** {interview_data.get('start_time', 'N/A')}
**End Time:** {interview_data.get('end_time', 'N/A')}
**Total Duration:** {interview_data.get('duration', 'N/A')}

## Session Statistics

- **Questions Asked:** {len(interview_data.get('questions', []))}
- **Responses Recorded:** {len(interview_data.get('responses', []))}
- **Average Response Time:** {interview_data.get('avg_response_time', 'N/A')}
- **Session Completion:** {'Complete' if interview_data.get('completed') else 'Incomplete'}

## Question Sequence

"""
        
        questions = interview_data.get('questions', [])
        for i, question in enumerate(questions, 1):
            report += f"{i}. **{question.get('skill_target', 'Unknown')}** ({question.get('difficulty', 'Unknown')})\n"
            report += f"   {question.get('question', 'No question text')[:100]}...\n\n"
        
        return report
    
    def generate_performance_analysis(self, 
                                    evaluations: List[Dict[str, Any]],
                                    skills_tested: List[str]) -> str:
        """
        Generate detailed performance analysis
        
        Args:
            evaluations (list): Question evaluations
            skills_tested (list): List of skills that were tested
            
        Returns:
            str: Performance analysis report
        """
        
        report = """
# Detailed Performance Analysis

## Skills Assessment Matrix

"""
        
        # Analyze performance by skill
        skill_performance = {}
        for evaluation in evaluations:
            if evaluation.get('success') and evaluation.get('evaluation'):
                skill = evaluation['evaluation'].get('skill_target', 'Unknown')
                score = evaluation['evaluation'].get('score', 0)
                grade = evaluation['evaluation'].get('grade', 'N/A')
                
                if skill not in skill_performance:
                    skill_performance[skill] = []
                skill_performance[skill].append({'score': score, 'grade': grade})
        
        for skill, performances in skill_performance.items():
            avg_score = sum(p['score'] for p in performances) / len(performances)
            grades = [p['grade'] for p in performances]
            
            report += f"### {skill}\n"
            report += f"- **Questions Asked:** {len(performances)}\n"
            report += f"- **Average Score:** {avg_score:.1f}/100\n"
            report += f"- **Grades:** {', '.join(grades)}\n\n"
        
        return report
    
    def export_to_json(self, data: Dict[str, Any], filename: str = None) -> str:
        """
        Export report data to JSON format
        
        Args:
            data (dict): Report data
            filename (str, optional): Output filename
            
        Returns:
            str: JSON formatted data
        """
        json_data = {
            "report_generated": datetime.now().isoformat(),
            "data": data
        }
        
        json_output = json.dumps(json_data, indent=2, ensure_ascii=False)
        
        if filename:
            try:
                with open(filename, 'w') as f:
                    f.write(json_output)
            except Exception as e:
                print(f"Error saving JSON file: {e}")
        
        return json_output
    
    def _candidate_feedback_template(self) -> str:
        """Template for candidate feedback reports"""
        return """
        # Candidate Feedback Template
        - Overall performance summary
        - Skill-by-skill breakdown
        - Specific strengths and improvements
        - Development recommendations
        - Next steps
        """
    
    def _hiring_manager_template(self) -> str:
        """Template for hiring manager reports"""
        return """
        # Hiring Manager Template
        - Executive summary
        - Recommendation with confidence level
        - Key performance metrics
        - Risk assessment
        - Next steps in hiring process
        """
    
    def _interview_summary_template(self) -> str:
        """Template for interview summary reports"""
        return """
        # Interview Summary Template
        - Session metadata
        - Question sequence and responses
        - Technical metrics
        - Completion status
        """
    
    def _performance_analysis_template(self) -> str:
        """Template for performance analysis reports"""
        return """
        # Performance Analysis Template
        - Skills matrix
        - Score distributions
        - Trend analysis
        - Comparative metrics
        """
    
    def generate_custom_report(self, 
                              template_name: str,
                              data: Dict[str, Any],
                              **kwargs) -> str:
        """
        Generate custom report using specified template
        
        Args:
            template_name (str): Name of template to use
            data (dict): Data for report generation
            **kwargs: Additional parameters
            
        Returns:
            str: Generated report
        """
        if template_name not in self.report_templates:
            return f"Template '{template_name}' not found"
        
        template_func = self.report_templates[template_name]
        return template_func()
    
    def create_summary_statistics(self, evaluations: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Create summary statistics from evaluations
        
        Args:
            evaluations (list): List of evaluation results
            
        Returns:
            dict: Summary statistics
        """
        if not evaluations:
            return {"error": "No evaluations provided"}
        
        successful_evals = [e for e in evaluations if e.get('success')]
        
        if not successful_evals:
            return {"error": "No successful evaluations"}
        
        scores = []
        grades = []
        skills = []
        
        for evaluation in successful_evals:
            eval_data = evaluation.get('evaluation', {})
            if 'score' in eval_data:
                scores.append(eval_data['score'])
            if 'grade' in eval_data:
                grades.append(eval_data['grade'])
            if 'skill_target' in eval_data:
                skills.append(eval_data['skill_target'])
        
        # Calculate statistics
        stats = {
            "total_questions": len(successful_evals),
            "average_score": sum(scores) / len(scores) if scores else 0,
            "highest_score": max(scores) if scores else 0,
            "lowest_score": min(scores) if scores else 0,
            "skills_tested": len(set(skills)),
            "unique_skills": list(set(skills))
        }
        
        # Grade distribution
        grade_counts = {}
        for grade in grades:
            grade_counts[grade] = grade_counts.get(grade, 0) + 1
        
        stats["grade_distribution"] = grade_counts
        
        # Performance level
        if stats["average_score"] >= 80:
            stats["performance_level"] = "Excellent"
        elif stats["average_score"] >= 70:
            stats["performance_level"] = "Good"
        elif stats["average_score"] >= 60:
            stats["performance_level"] = "Fair"
        else:
            stats["performance_level"] = "Needs Improvement"
        
        return stats

================
File: utils\tool_logger.py
================
"""
Enhanced Tool Logging System for tracking all tool calls and responses
"""

import logging
import json
import time
from datetime import datetime
from typing import Dict, Any, Callable, Optional
from functools import wraps

# Configure tool-specific logger
tool_logger = logging.getLogger('tool_calls')
tool_handler = logging.StreamHandler()
tool_formatter = logging.Formatter(
    '%(asctime)s - TOOL_CALL - %(levelname)s - %(message)s'
)
tool_handler.setFormatter(tool_formatter)
tool_logger.addHandler(tool_handler)
tool_logger.setLevel(logging.INFO)

class ToolLogger:
    """
    Comprehensive logging system for all tool calls and responses
    """
    
    def __init__(self):
        self.call_history = []
        self.performance_metrics = {}
        
    def log_tool_call(self, 
                     tool_name: str, 
                     agent_name: str,
                     input_params: Dict[str, Any], 
                     response: Any, 
                     execution_time: float,
                     success: bool,
                     error: str = None) -> None:
        """
        Log detailed information about tool calls
        
        Args:
            tool_name (str): Name of the tool called
            agent_name (str): Name of the agent making the call
            input_params (dict): Parameters passed to the tool
            response (Any): Tool response
            execution_time (float): Time taken to execute
            success (bool): Whether the call was successful
            error (str): Error message if failed
        """
        
        call_record = {
            'timestamp': datetime.now().isoformat(),
            'tool_name': tool_name,
            'agent_name': agent_name,
            'input_params': self._sanitize_params(input_params),
            'response_summary': self._summarize_response(response),
            'execution_time_ms': round(execution_time * 1000, 2),
            'success': success,
            'error': error
        }
        
        self.call_history.append(call_record)
        
        # Update performance metrics
        if tool_name not in self.performance_metrics:
            self.performance_metrics[tool_name] = {
                'total_calls': 0,
                'successful_calls': 0,
                'failed_calls': 0,
                'total_execution_time': 0,
                'average_execution_time': 0
            }
        
        metrics = self.performance_metrics[tool_name]
        metrics['total_calls'] += 1
        metrics['total_execution_time'] += execution_time
        metrics['average_execution_time'] = metrics['total_execution_time'] / metrics['total_calls']
        
        if success:
            metrics['successful_calls'] += 1
        else:
            metrics['failed_calls'] += 1
        
        # Log to console
        log_message = f"TOOL: {tool_name} | AGENT: {agent_name} | TIME: {execution_time*1000:.2f}ms | SUCCESS: {success}"
        if error:
            log_message += f" | ERROR: {error}"
            
        tool_logger.info(log_message)
        
        # Detailed parameter logging
        tool_logger.debug(f"TOOL PARAMS: {json.dumps(self._sanitize_params(input_params), indent=2)}")
        
        if not success:
            tool_logger.error(f"TOOL FAILURE - {tool_name}: {error}")
    
    def _sanitize_params(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """Sanitize parameters for logging (truncate long strings, etc.)"""
        sanitized = {}
        for key, value in params.items():
            if isinstance(value, str) and len(value) > 200:
                sanitized[key] = value[:200] + "... (truncated)"
            elif isinstance(value, dict):
                sanitized[key] = {k: str(v)[:100] if isinstance(v, str) else v for k, v in value.items()}
            else:
                sanitized[key] = value
        return sanitized
    
    def _summarize_response(self, response: Any) -> Dict[str, Any]:
        """Create a summary of the response for logging"""
        if isinstance(response, str):
            return {
                'type': 'string',
                'length': len(response),
                'preview': response[:100] + "..." if len(response) > 100 else response
            }
        elif isinstance(response, dict):
            return {
                'type': 'dict',
                'keys': list(response.keys()),
                'size': len(response)
            }
        elif response is None:
            return {'type': 'None'}
        else:
            return {
                'type': str(type(response).__name__),
                'str_representation': str(response)[:100]
            }
    
    def get_performance_report(self) -> Dict[str, Any]:
        """Get comprehensive performance report of all tool calls"""
        total_calls = sum(metrics['total_calls'] for metrics in self.performance_metrics.values())
        successful_calls = sum(metrics['successful_calls'] for metrics in self.performance_metrics.values())
        
        return {
            'summary': {
                'total_calls': total_calls,
                'successful_calls': successful_calls,
                'failed_calls': total_calls - successful_calls,
                'success_rate': (successful_calls / total_calls * 100) if total_calls > 0 else 0,
                'tools_used': len(self.performance_metrics)
            },
            'per_tool_metrics': self.performance_metrics,
            'recent_calls': self.call_history[-10:]  # Last 10 calls
        }
    
    def get_tool_usage_summary(self, tool_name: str) -> Dict[str, Any]:
        """Get usage summary for a specific tool"""
        if tool_name not in self.performance_metrics:
            return {'error': f'No data for tool: {tool_name}'}
        
        metrics = self.performance_metrics[tool_name]
        tool_calls = [call for call in self.call_history if call['tool_name'] == tool_name]
        
        return {
            'tool_name': tool_name,
            'metrics': metrics,
            'recent_calls': tool_calls[-5:],  # Last 5 calls for this tool
            'common_errors': self._get_common_errors(tool_calls)
        }
    
    def _get_common_errors(self, tool_calls: list) -> Dict[str, int]:
        """Analyze common errors for a tool"""
        error_counts = {}
        for call in tool_calls:
            if not call['success'] and call['error']:
                error_type = call['error'][:50]  # First 50 chars of error
                error_counts[error_type] = error_counts.get(error_type, 0) + 1
        return error_counts

# Global logger instance
tool_logger_instance = ToolLogger()

def log_tool_call(tool_name: str, agent_name: str = "Unknown"):
    """
    Decorator for logging tool calls with detailed metrics
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = time.time()
            success = False
            error = None
            response = None
            
            try:
                response = func(*args, **kwargs)
                success = True
                return response
            except Exception as e:
                error = str(e)
                raise
            finally:
                execution_time = time.time() - start_time
                
                # Extract meaningful parameters
                input_params = {
                    'args_count': len(args),
                    'kwargs': kwargs
                }
                
                tool_logger_instance.log_tool_call(
                    tool_name=tool_name,
                    agent_name=agent_name,
                    input_params=input_params,
                    response=response,
                    execution_time=execution_time,
                    success=success,
                    error=error
                )
        
        return wrapper
    return decorator

================
File: utils\web_search.py
================
"""
Web Search Tool for agents to research Excel best practices and current information
"""

import requests
from typing import Dict, List, Any, Optional
import logging

logger = logging.getLogger(__name__)

class WebSearchTool:
    """
    Web search tool using DuckDuckGo API for researching Excel information
    """
    
    def __init__(self):
        self.base_url = "https://api.duckduckgo.com/"
        self.headers = {
            'User-Agent': 'ExcelInterviewer/1.0 (Educational Tool)'
        }
    
    def search_excel_topic(self, query: str, max_results: int = 3) -> Dict[str, Any]:
        """
        Search for Excel-related information
        
        Args:
            query (str): Search query
            max_results (int): Maximum number of results to return
            
        Returns:
            dict: Search results and metadata
        """
        try:
            # Enhance query with Excel context if not present
            if "excel" not in query.lower():
                enhanced_query = f"Microsoft Excel {query}"
            else:
                enhanced_query = query
            
            # Simple web search implementation
            # In a real implementation, you'd use a proper search API
            results = self._perform_search(enhanced_query, max_results)
            
            return {
                "success": True,
                "query": enhanced_query,
                "results": results,
                "total_results": len(results)
            }
            
        except Exception as e:
            logger.error(f"Web search failed: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "query": query,
                "results": []
            }
    
    def _perform_search(self, query: str, max_results: int) -> List[Dict[str, str]]:
        """
        Perform the actual web search
        
        Args:
            query (str): Search query
            max_results (int): Maximum results
            
        Returns:
            list: Search results
        """
        # Mock search results for Excel topics
        # In production, integrate with a real search API
        mock_results = [
            {
                "title": f"Excel Guide: {query}",
                "url": "https://support.microsoft.com/excel",
                "snippet": f"Comprehensive guide covering {query} with step-by-step instructions and examples.",
                "source": "Microsoft Support"
            },
            {
                "title": f"Best Practices for {query}",
                "url": "https://excel-university.com",
                "snippet": f"Professional tips and best practices for implementing {query} in Excel workbooks.",
                "source": "Excel University"
            },
            {
                "title": f"Advanced {query} Techniques",
                "url": "https://chandoo.org",
                "snippet": f"Advanced techniques and formulas for mastering {query} in Excel.",
                "source": "Chandoo.org"
            }
        ]
        
        return mock_results[:max_results]
    
    def search_excel_function(self, function_name: str) -> Dict[str, Any]:
        """
        Search for specific Excel function information
        
        Args:
            function_name (str): Excel function name
            
        Returns:
            dict: Function information
        """
        query = f"Excel {function_name} function syntax examples"
        return self.search_excel_topic(query, max_results=2)
    
    def search_excel_best_practices(self, topic: str) -> Dict[str, Any]:
        """
        Search for Excel best practices on a specific topic
        
        Args:
            topic (str): Topic for best practices
            
        Returns:
            dict: Best practices information
        """
        query = f"Excel best practices {topic}"
        return self.search_excel_topic(query, max_results=3)
    
    def search_troubleshooting(self, issue: str) -> Dict[str, Any]:
        """
        Search for troubleshooting information
        
        Args:
            issue (str): Excel issue to troubleshoot
            
        Returns:
            dict: Troubleshooting information
        """
        query = f"Excel troubleshoot fix {issue}"
        return self.search_excel_topic(query, max_results=2)

================
File: utils\__init__.py
================
"""
AI-Powered Excel Mock Interviewer - Utils Module
"""

from .prompts import (
    QUERAFT_SYSTEM_PROMPT,
    REVIEWER_SYSTEM_PROMPT,
    RECRUITER_SYSTEM_PROMPT,
    INTERVIEWER_SYSTEM_PROMPT
)
from .json_parser import JSONParser
from .mock_data_generator import MockDataGenerator
from .web_search import WebSearchTool
from .report_generator import ReportGenerator

__all__ = [
    'QUERAFT_SYSTEM_PROMPT',
    'REVIEWER_SYSTEM_PROMPT', 
    'RECRUITER_SYSTEM_PROMPT',
    'INTERVIEWER_SYSTEM_PROMPT',
    'JSONParser',
    'MockDataGenerator',
    'WebSearchTool',
    'ReportGenerator'
]
